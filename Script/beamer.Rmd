---
title: "Traitement des questions ouvertes avec R"
author: "Paul BALAFAI et Mame Balla BOUSSO"
date: "`r Sys.Date()`"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "dolphin"
    fonttheme: "structurebold"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# \textcolor{purple}{\textbf{\textit{Plan de pr√©sentation}}}


- [Traitement des questions ouvertes avec R](#analyse-textuelle-reponses-enquete)
  - [1. Importation et Nettoyage des Donn√©es](#importation-nettoyage-donnees)
  - [2. Exploration et Pr√©traitement Textuel](#exploration-pretraitement-textuel)
  - [3. Analyse Th√©matique (LDA)](#analyse-thematique-lda)
  - [4. Approche Alternative avec BERTopic](#approche-alternative-bertopic)
  - [5. Cat√©gorisation](#visualisation)
  - [CONCLUSION](#conclusion)
- [R√©f√©rences](#references-webographiques)


# \textcolor{purple}{\textbf{\textit{INTRODUCTION}}}


Le traitement automatique du langage naturel (TALN) regroupe un ensemble de techniques permettant d‚Äôanalyser, de comprendre et de transformer des textes en donn√©es exploitables. 

La forme la plus courante est l‚Äôanalyse **supervis√©e**, o√π chaque texte est associ√© √† un label pr√©d√©fini. Ces labels peuvent par exemple repr√©senter des cat√©gories binaires comme 0 ou 1, ou encore des sentiments comme positif, n√©gatif ou neutre.

Cependant, dans de nombreux cas, notamment dans les **questions ouvertes d‚Äôenqu√™tes**, il n'existe aucune annotation pr√©alable permettant de guider l‚Äôapprentissage. Il devient alors n√©cessaire de structurer les donn√©es sans rep√®re pr√©alable, en regroupant les textes selon leur **similarit√© s√©mantique**. 
Dans cette √©tude, nous nous concentrerons sp√©cifiquement sur cette approche **non supervis√©e**.

Packages : **topicmodels (pour le mod√®le LDA)**, **tidytext**, et **BERTopic** (**reticulate**) .


# \textcolor{purple}{\textbf{\textit{Importation des donn√©es}}}

##  package

```{r message=FALSE, warning=FALSE, echo=TRUE}
library(haven)
library(readxl)       # Pour lire les fichiers Excel
library(topicmodels)  # Pour la mod√©lisation th√©matique
library(ggplot2)      # Pour les visualisations
library(dplyr)        # Pour la manipulation de donn√©es
library(tidytext)     # Pour le traitement de texte
library(tidyr)        # Pour la gestion des donn√©es
library(wordcloud)    # Pour les nuages de mots
library(tidyverse)    # Collection de packages pour la science des donn√©es
library(tm)           # Pour le text mining
library(SnowballC)    # Pour le stemming
library(stringr)      # Pour la manipulation de strings

```

# \textcolor{purple}{\textbf{\textit{}}}

##  Importation des donn√©es

```{r, warning=FALSE, message=FALSE}

Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration_ <- read_excel("Data/Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration).xlsx")
Texte_JI <- Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration_
head(Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration_)

autres_rgph5 <- read_dta("Data/autres_rgph5.dta")
colnames(autres_rgph5)


data_facteurs <- data.frame(lapply(autres_rgph5, haven::as_factor))
nrow(data_facteurs)
ids_data <- data_facteurs$id

# Garde uniquement les documents non vides pour le LDA
data_loge_filtered <- data_facteurs %>%
  select(id, E01, E01_AUTRE) %>%
  filter(E01 == "Autre")
```

# \textcolor{purple}{\textbf{\textit{}}}

## base rgph

```{r}
unique(data_facteurs$E01)
```


# \textcolor{purple}{\textbf{\textit{}}}


#### V√©rification des colonnes

```{r}
colnames(Texte_JI)
```

### Identification des textes vides

```{r, warning=FALSE, message=FALSE}

text_id_empty <- Texte_JI %>%
  group_by(id) %>%
  summarise(nb_mots = sum(!is.na(Texte))) %>%
  filter(nb_mots == 0) %>%
  pull(id)
Texte_JI_filtered <- Texte_JI %>% 
  filter(!(id %in% text_id_empty))

head(Texte_JI_filtered)
nrow(Texte_JI_filtered)
```


# \textcolor{purple}{\textbf{\textit{}}}

On remarque que le nombre de ligne a diminu√© passant de 128 √† 45. Seulement 45 lignes contiennent des textes.

## Nettoyage des textes

On cr√©e une fonction pour traiter les textes afin de faciliter leur analyse

```{r, warning=FALSE, message=FALSE}
clean_text <- function(text) {
  text <- tolower(text)           # Conversion en minuscules
  text <- removePunctuation(text) # Suppression de la ponctuation
  text <- removeNumbers(text)     # Suppression des chiffres
  text <- stripWhitespace(text)   # Suppression des espaces superflus
  return(text)
}

data <- Texte_JI_filtered %>%
  mutate(Texte_corrige = sapply(Texte, clean_text))

data1 <- data_loge_filtered

```


# \textcolor{purple}{\textbf{\textit{2. Exploration et Pr√©traitement des textes}}}

Dans le processus de pr√©traitement des donn√©es, on va tokeniser la base de donn√©es pour analyser non pas les textes, mais les mots directement.

```{r, warning=FALSE, message=FALSE}

tokenized_textes <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(input = 'Texte_corrige', output = 'word')

tokenized_textes %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col()  + 
  labs(title = "Les mots apparaissant plus de 5 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))

# Nombre total de lignes apr√®s tokenisation
nrow(tokenized_textes)

```


# \textcolor{purple}{\textbf{\textit{}}}
```{r, warning=FALSE, message=FALSE}
# Nombre total de lignes apr√®s tokenisation
nrow(tokenized_textes)
```

De nombreux mots pr√©sents n‚Äôapportent aucune r√©elle valeur √† notre analyse. Des mots comme **de**, **pour**, **les**, **le**, **la** sont ce qu‚Äôon appelle des mots vides (stop words).

Nous allons supprimer ces mots en utilisant la commande *anti_join(stop_words)*.


# \textcolor{purple}{\textbf{\textit{}}}

### cas de la base rgph (Autre √† pr√©ciser dans type de logement)

```{r, warning=FALSE, message=FALSE}
data1 <- data_loge_filtered %>%
  mutate(Texte_corrige1 = sapply(E01_AUTRE, clean_text))

tokenized_textes1 <- data1 %>%
  select(id, Texte_corrige1) %>%
  unnest_tokens(input = 'Texte_corrige1', output = 'word')

```


# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
tokenized_textes1 %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 1000) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col()  + 
  labs(title = "Les mots apparaissant plus de 1000 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))
```


# \textcolor{purple}{\textbf{\textit{}}}

### Charger les stop words en fran√ßais

```{r, warning=FALSE, message=FALSE}

stop_words_fr <- tibble(word = stopwords("fr")) 
head(stop_words_fr, 10)

```


# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
Stop_texte <- tokenized_textes %>% 
  anti_join(stop_words_fr, by = "word")

nrow(Stop_texte)
```
On voit bien que le nombre de mots diminue suite √† la supression des 
stop word.
Comme nous pouvons le voir sur le graphique ci-dessous, il reste moins de mots, mais ils sont beaucoup plus pertinents pour l‚Äôanalyse.


# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
Stop_texte %>%
  anti_join(stop_words_fr) %>% # trouve l√† o√π les textes rencontrent des stop words, et les supprime
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col() + 
  labs(title = "Les mots apparaissant plus de 5 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))
```


# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
library(ggwordcloud) # Une autre mani√®re de visualiser

Stop_texte %>%
  anti_join(stop_words_fr) %>%
  count(word, sort = TRUE) %>%
  filter(n > 4) %>%
  ggplot(aes(label = word, size = n, color = n)) + 
  geom_text_wordcloud() + 
  scale_size_area(max_size = 15) 

```


# \textcolor{purple}{\textbf{\textit{}}}

L'application des stop word diminue le nombre de mots. Ceci le montre

```{r, warning=FALSE, message=FALSE}

Stop_texte %>%
  count(word, sort = TRUE)%>%
  nrow()

```

# \textcolor{purple}{\textbf{\textit{}}}

## Cas du rgph5

```{r}
Stop_texte1 <- tokenized_textes1 %>% 
  anti_join(stop_words_fr, by = "word")

Stop_texte1 %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  slice_max(order_by = count, n = 10) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col(fill = "steelblue") + 
  labs(title = "Les 10 mots les plus fr√©quents",
       x = "Fr√©quence",
       y = "Mot") + 
  theme_minimal()
```

# \textcolor{purple}{\textbf{\textit{}}}

### Racinisation
En racinisant, les mots *cultures* et *culture* par exemple se r√©duisent en *culture*. Voil√† pourquoi le nombre total de mots diminue comme le r√©sultat de cette commande l'illustre :

```{r, warning=FALSE, message=FALSE}
# stemming
Stop_texte = Stop_texte %>%
  mutate(stem = wordStem(word))

# unique count of words after stemming
Stop_texte %>%
  count(stem, sort = TRUE) %>%
  nrow()

```


# \textcolor{purple}{\textbf{\textit{}}}
Fr√©quence des mots avant le stemming

```{r, warning=FALSE, message=FALSE}
# Fr√©quence des mots avant le stemming
Stop_texte %>%
  count(word) %>%
  arrange(desc(n)) %>%
  head(10)

```

Fr√©quence des mots apr√®s le stmming

```{r, warning=FALSE, message=FALSE}
# Fr√©quence des mots apr√®s le stemming
Stop_texte %>%
  count(stem) %>%
  arrange(desc(n)) %>%
  head(10)

```


# \textcolor{purple}{\textbf{\textit{}}}

On remarque que le stemming ne semble pas respecter la logique pour certains mots.
En effet, la racinisation supprime les lettres *s* √† la fin des mots comme *plus* et *temps*.
Egalement le mot *paix* est r√©duit √† *pai*. Cela constitut une limite majeure quant √† la racinisation en langue fran√ßaise. C'est pourquoi dans ce qui suit, nous ferons fi de cette √©tape du pr√©traitement en utilisanat d√©sormais seulement mes textes issus de l'application des stop word.

# \textcolor{purple}{\textbf{\textit{}}}

### les analyse TF-IDF

Ci-dessous, nous voyons l'int√©gralit√© du tableau TF-IDF. 
Ce qui nous int√©resse le plus, c'est la colonne tf_idf, car elle nous donne le classement pond√©r√© ou l'importance des mots dans notre texte.

```{r, warning=FALSE, message=FALSE}

texte_tf_idf <- Stop_texte %>%
  count(word, id, sort = TRUE) %>%
  rename(count = n) %>%
  bind_tf_idf(word, id, count)
head(texte_tf_idf)


```

# \textcolor{purple}{\textbf{\textit{}}}

Les simples d√©comptes de fr√©quences de mots peuvent √™tre trompeurs et peu utiles
pour bien comprendre nos donn√©es. Il est en fait int√©ressant de voir les mots les plus fr√©quents dans chaque texte.

# \textcolor{purple}{\textbf{\textit{}}}


```{r}
texte_tf_idf1 <- Stop_texte1 %>%
  count(word, id, sort = TRUE) %>%
  rename(count = n) %>%
  bind_tf_idf(word, id, count)
head(texte_tf_idf)
```

# \textcolor{purple}{\textbf{\textit{}}}

Les simples d√©comptes de fr√©quences de mots peuvent √™tre trompeurs et peu utiles
pour bien comprendre nos donn√©es. Il est en fait int√©ressant de voir les mots les plus 
fr√©quents dans chaque texte.

```{r, warning=FALSE, message=FALSE}

texte_tf_idf %>%
  select(word, id, tf_idf, count) %>%
  group_by(id) %>%
  slice_max(order_by = count, n = 6, with_ties=FALSE) %>% #takes top 5 words from each text
  filter(id < 6) %>% #just look at 5 textes
  ggplot(aes(label = word)) + 
  geom_text_wordcloud() + 
  facet_grid(rows = vars(id))

```

# \textcolor{purple}{\textbf{\textit{}}}

On s'est limit√© au cinq premiers textes. Mais les textes correspondant aux identifiants 1 et 3 sont des NA et donc ont √©t√© isol√©s.
Par ailleurs le r√©sultat qui suit montre aussi qu'il ne faut pas se limiter √† un simple d√©nombrement des textes mais √† leur fr√©quence.

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}

texte_tf_idf %>%
  select(word, id, tf_idf) %>%
  group_by(id) %>%
  slice_max(order_by = tf_idf,n = 6, with_ties=FALSE) %>% #takes top 5 words from each text
  filter(id < 6) %>% #just look at 5 texts
  ggplot(aes(label = word)) + 
  geom_text_wordcloud() + 
  facet_grid(rows = vars(id))

```

# \textcolor{purple}{\textbf{\textit{}}}

### Relations entre les mots

Jusqu'√† pr√©sent, nous avons seulement examin√© les mots individuellement. Mais que faire si nous voulons conna√Ætre les relations entre les mots dans un texte ? 
Cela peut √™tre accompli gr√¢ce aux n-grammes.

```{r, warning=FALSE, message=FALSE}

textes_bigram <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2) 
head(textes_bigram)

```

# \textcolor{purple}{\textbf{\textit{}}}


Comme vous pouvez le voir dans le dataframe ci-dessus, certains bigrammes contiennent des mots vides (stop words) qui n‚Äôapportent pas beaucoup de valeur. 
Supprimons ces mots vides. Pour cela, nous allons d'abord s√©parer la colonne des bigrammes en deux colonnes distinctes nomm√©es 'word1' et 'word2'. 
Ensuite, nous utiliserons deux fonctions de filtre pour supprimer les mots vides.


# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
textes_bigram <- textes_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%#separates on whitespace
  filter(!word1 %in% stop_words_fr$word) %>%
  filter(!word2 %in% stop_words_fr$word)

head(textes_bigram)

```


# \textcolor{purple}{\textbf{\textit{}}}
On peut maintenant compter les bigram et voir le r√©sultat

```{r, warning=FALSE, message=FALSE}
bigram_counts <- textes_bigram %>%
  count(word1, word2, sort = TRUE)
head(bigram_counts)
```

Comme pr√©c√©demment, on peut aussi cr√©er une mesure TF-IDF avec des n-grammes.
Faisons-le maintenant.

# \textcolor{purple}{\textbf{\textit{}}}
```{r, warning=FALSE, message=FALSE}
data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2) %>%
  count(id, bigram) %>%
  bind_tf_idf(bigram, id, n) %>%
  group_by(id) %>%
  arrange(id, desc(tf_idf)) %>%
  head()

```

Comme on peut le voir ci-dessus, beaucoup de valeurs TF-IDF sont identiques.
Cela est en partie d√ª √† la petite taille des textes.

# \textcolor{purple}{\textbf{\textit{}}}

### Cas du rgph5

```{r, warning=FALSE, message=FALSE}
data1 %>%
  select(id, Texte_corrige1) %>%
  unnest_tokens(bigram, Texte_corrige1, token = 'ngrams', n = 2) %>%
  count(id, bigram) %>%
  bind_tf_idf(bigram, id, n) %>%
  group_by(id) %>%
  arrange(id, desc(tf_idf)) %>%
  head()
```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
library('igraph')
library('ggraph')
bi_graph <- bigram_counts %>%
  filter(n > 1) %>% 
  graph_from_data_frame()

ggraph(bi_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

# \textcolor{purple}{\textbf{\textit{}}}

Comme on peut le voir ci-dessus, de nombreux noms et d‚Äôautres 
informations ont √©t√© extraits des donn√©es.

### Cas des trigrams

# \textcolor{purple}{\textbf{\textit{}}}

On peut aussi voir ci-dessous les trigrams

```{r, warning=FALSE, message=FALSE}
texte_trigram <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% #separates on whitespace
  filter(!word1 %in% stop_words_fr$word) %>%
  filter(!word2 %in% stop_words_fr$word) %>%
  filter(!word3 %in% stop_words_fr$word)

head(texte_trigram)
```


# \textcolor{purple}{\textbf{\textit{}}}


```{r, warning=FALSE, message=FALSE}
trigram_counts <- texte_trigram %>%
  count(word1, word2, word3, sort = TRUE)
head(trigram_counts)
```

Comme pr√©c√©demment, on peut aussi cr√©er une mesure TF-IDF avec des trigrammes. Faisons-le maintenant.

# \textcolor{purple}{\textbf{\textit{}}}


```{r, warning=FALSE, message=FALSE}
data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
  count(id, trigram) %>%
  bind_tf_idf(trigram, id, n) %>%
  group_by(id) %>%
  arrange(id, desc(tf_idf)) %>%
  head()
```

Beaucoup de valeurs TF-IDF sont identiques.
Cela est en partie d√ª √† la petite taille des textes comme remarqu√© dans le cas bigram.
  
# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
tri_graph <- trigram_counts %>%
  filter(n > 0) %>% # Ici, on garde TOUS les trigrammes pr√©sents au moins une fois
  graph_from_data_frame()
ggraph(tri_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

# \textcolor{purple}{\textbf{\textit{}}}

### Remarque 

Normalement, on mettrait n > 1 ou n > 2 pour filtrer les trigrammes peu fr√©quents, mais dans notre cas, les textets sont tr√®s courts, donc les trigrammes se r√©p√®tent tr√®s peu.

# \textcolor{purple}{\textbf{\textit{}}}

Dans notre base de donn√©es, le champ contenant les suggestions n‚Äôest pas obligatoire, ce qui signifie que plusieurs enregistrements pr√©sentent des valeurs manquantes (NA).

```{r, warning=FALSE, message=FALSE}
# 1. Extraire les ID avant pr√©traitement (Texte_JI)
Id_initial_texte <- unique(Texte_JI$id)
length(Id_initial_texte)

# 2. Extraire les ID apr√®s pr√©traitement (Stop_texte)
Id_final_texte <- unique(Stop_texte$id)
length(Id_final_texte)

#. Identifier les tweets manquants (pr√©sents avant, absents apr√®s)
Id_texte_NA <- setdiff(Id_initial_texte, Id_final_texte)
length(Id_texte_NA)
```

# \textcolor{purple}{\textbf{\textit{3. Analyse Th√©matique (LDA)}}}
La m√©thode LDA (Latent Dirichlet Allocation).
LDA repose sur deux grands principes :
Chaque document est un m√©lange de plusieurs sujets. Chaque sujet est un m√©lange de mots

Un exemple classique serait de supposer qu‚Äôil existe deux grands sujets dans 
les actualit√©s : la politique et le divertissement.
Le sujet politique contiendra des mots comme √©lu, gouvernement, tandis que le sujet divertissement contiendra des mots comme film, acteur.
Mais certains mots peuvent appara√Ætre dans les deux, comme prix ou budget.
LDA va identifier :
les m√©langes de mots qui composent chaque sujet, et
les m√©langes de sujets qui composent chaque document.
   
# \textcolor{purple}{\textbf{\textit{}}}
```{r, warning=FALSE, message=FALSE}
# cr√©ation d'une matrice document-th√®me pour LDA
df_dtm <- Stop_texte %>%
  count(id, word) %>%              
  cast_dtm(id, word, n)

df_dtm1 <- Stop_texte1 %>%
  mutate(id = as.character(id)) %>%
  count(id, word) %>%              
  cast_dtm(id, word, n)

```

## Choix du nombre k de th√®mes

Dans le cadre de la mod√©lisation th√©matique avec LDA (Latent Dirichlet Allocation), un des √©l√©ments cl√©s du param√©trage est le choix du nombre de th√®mes (K).
Ce param√®tre n‚Äôest pas d√©termin√© automatiquement par le mod√®le ; il doit √™tre choisi par l‚Äôutilisateur, en fonction des donn√©es et des objectifs de l‚Äôanalyse.
Or, le nombre de th√®mes a un impact direct sur la qualit√© et la lisibilit√© du mod√®le.

# \textcolor{purple}{\textbf{\textit{}}}

### Application de la perpelexit√© pour le choix de K

```{r, warning=FALSE, message=FALSE}
k_values <- 2:10
perplexities <- sapply(k_values, function(k) {
  lda_model <- LDA(df_dtm, k = k, control = list(seed = 1234))
  perplexity(lda_model)
})

# Afficher le graphique
ggplot(data.frame(K = k_values, Perplexity = perplexities), aes(x = K, y = Perplexity)) +
  geom_point() + geom_line() +
  labs(title = "Choix du K optimal avec la perplexit√©",
       x = "Nombre de th√®mes (K)", y = "Perplexit√©")


```


# \textcolor{purple}{\textbf{\textit{}}}

Cas du rgph5

```{r, warning=FALSE, message=FALSE}
k_values <- 2:10
perplexities <- sapply(k_values, function(k) {
  lda_model <- LDA(df_dtm1, k = k, control = list(seed = 1234))
  perplexity(lda_model)
})

# Afficher le graphique
ggplot(data.frame(K = k_values, Perplexity = perplexities), aes(x = K, y = Perplexity)) +
  geom_point() + geom_line() +
  labs(title = "Choix du K optimal avec la perplexit√©",
       x = "Nombre de th√®mes (K)", y = "Perplexit√©")


```

# \textcolor{purple}{\textbf{\textit{}}}


### Cas o√π le graphe de la perpel√©xit√© ne produit pas un r√©sultat escompt√© (comme celui du rgph5)

```{r message=FALSE, warning=FALSE}

text_df2 <- Stop_texte %>%
  mutate(text_semantic = word) %>%  # dupliquer la colonne lemmatis√©e
  mutate(
    text_semantic = str_replace_all(text_semantic, "\\b(manger|plat|mets|piment|restaurant)\\b", "alimentation"),
    text_semantic = str_replace_all(text_semantic, "\\b(sketch|micro|temps|chanter|court|courtm√©trage|sketcheval|sc√®ne|culture|culturel|lapprentissage|salle|pr√©sentation|apprendre|discours|m√©trage|fun|espace|prestataire|marrant|comique|communaut√©|am√©nager)\\b", "animation"),
    text_semantic = str_replace_all(text_semantic, "\\b(audible|sonorisation|technique)\\b", "technique"),
    text_semantic = str_replace_all(text_semantic, "\\b(communication|organisation|cr√©ativit√©|organiser)\\b", "Organisation")
  )

```

#### Limites : pas trop flexible surtout en cas de grands volumes de donn√©es

# \textcolor{purple}{\textbf{\textit{}}}

### Une autre solution (validation humaine)

tokenized_textes1 %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 1000) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col()  + 
  labs(title = "Les mots apparaissant plus de 1000 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))


# \textcolor{purple}{\textbf{\textit{}}}

* Isolement des termes r√©currents √† partir de la visualisation

* Regroupement des mots de m√™me sens

* Poursuivre le processus pour avoir une base compos√©s uniquement des mots isol√©s (ceux que l'on g√®re bien)


# \textcolor{purple}{\textbf{\textit{}}}

```{r message=FALSE, include=FALSE, echo=TRUE}
mots_cles <- c("mot1", "mot2", "...")

tokens_filtres <- Stop_texte %>%
  filter(word %in% mots_cles)

restants <- Stop_texte %>%
  anti_join(tokens_filtres, by = c("id", "word"))
```


# \textcolor{purple}{\textbf{\textit{}}}

### Lancement du mod√®le


```{r, warning=FALSE, message=FALSE, echo=TRUE}
lda_model <- LDA(df_dtm, k = 8, control = list(seed = 1234))
# Termes par th√®me
terms_by_topic <- tidy(lda_model, matrix = "beta")
```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}

terms_by_topic
```

La colonne beta repr√©sente la probabilit√© qu‚Äôun mot donn√© appartienne √† un th√®me particulier.

# \textcolor{purple}{\textbf{\textit{}}}
```{r, warning=FALSE, message=FALSE}
top_terms <- terms_by_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%  # Prend exactement 10 termes par th√®me
  ungroup() %>%
  arrange(topic, -beta)  # Trie par th√®me et par probabilit√© d√©croissante

# V√©rification du nombre de termes s√©lectionn√©s par th√®me
top_terms %>%
  count(topic) 
```


# \textcolor{purple}{\textbf{\textit{Analyse en Composantes Principales (ACP)}}}

```{r, warning=FALSE, message=FALSE}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Termes les plus probables par th√®me",
       x = "Probabilit√© (beta)", y = "Terme")

```


# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
# √âtape 4 : Classification des textes par th√®me
textes_gamma <- tidy(lda_model, matrix = "gamma")
# Afficher les textes avec leur th√®me dominant
textes_classified <- textes_gamma %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

```


Ici pour chaque texte, on peut voir la probabilit√© qu'il a d'appartenir √† chacun des th√®mes.

# \textcolor{purple}{\textbf{\textit{}}}


```{r, warning=FALSE, message=FALSE}
# Nombre de texte dans chaque th√®me
textes_classified %>%
  count(topic)

```
Pour chaque th√®me, on voit le nombre de textes

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
# Visualisation des nombres de texte dans chaque th√®me
textes_classified %>%
  ggplot(aes(factor(topic), fill = factor(topic))) +
  geom_bar() +
  labs(title = "Distribution des th√®mes dominants",
       x = "Th√®me", y = "Nombre de textes")
```

Nous avons le nombbre de textes pour chaque th√®me

# \textcolor{purple}{\textbf{\textit{}}}

### Labellisation (tr√®s subjective)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
# Labelliser les th√®mes

textes_gamma <- textes_gamma %>%
  mutate(topic_label = case_when(
    topic == 1 ~ "Diversit√© culturelle et activit√©s communes",
    topic == 2 ~ "Performances artistiques et intervention sc√©nique",
    topic == 3 ~ "Pr√©sentation des cultures par les communaut√©s",
    topic == 4 ~ "Am√©nagement de l'espace et la gestion du temps",
    topic == 5 ~ "Organisation",
    topic == 6 ~ "Suggestions d'am√©lioration",
    topic == 7 ~ "Organisation g√©n√©rale et impression globale",
    topic == 8 ~ "animation",
    TRUE ~ "Autre"
  ))
```

# \textcolor{purple}{\textbf{\textit{4. Approche Alternative avec BERTopic}}}
BERTopic est un outil puissant de topic modeling (mod√©lisation de sujets) 
qui permet d‚Äôextraire automatiquement des th√®mes principaux √† partir de 
textes non structur√©s. Il se distingue des approches classiques comme 
LDA par sa capacit√© √† capturer des relations s√©mantiques en se basant sur
le texte et non des motstokenis√©s.


# \textcolor{purple}{\textbf{\textit{Configuration pour travail en python}}}

```{r message=FALSE, warning=FALSE, include=FALSE, echo=TRUE}
library(reticulate)
# Voir l'environnement actif de reticulate
py_config()
# Installation des packages n√©cessaires dans l‚Äôenvironnement actif de reticulate
reticulate::py_install(
  packages = c("sentence-transformers", "hdbscan", "umap-learn", "bertopic"),
  pip = TRUE
)

```


# \textcolor{purple}{\textbf{\textit{}}}

### Importation des modules Python

Chaque module que nous importons est li√© √† une fonctionnalit√© cl√© :

- sentence_transformers : gestion des mod√®les d'embedding de texte
- hdbscan : algorithme de clustering utilis√© par BERTopic
- bertopic : la librairie principale pour la mod√©lisation de sujets
- umap : utilis√© pour projeter les embeddings dans un espace de plus faible dimension

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE, echo=TRUE}
sentence_transformers <- import("sentence_transformers")
hdbscan <- import("hdbscan")
bertopic <- import("bertopic")
umap <- import("umap")  # important pour fixer le random_state
```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
# üî§ Mod√®le d'embedding (changeable par d'autres plus bas)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-MiniLM-L6-v2")

```

Ici on utilise 'paraphrase-MiniLM-L6-v2', un mod√®le rapide et efficace. Mais
il existe d'autres vari√©t√©s plus puissantes mais qui sont plus robustes en m√©moire
Le tableau qui suit donne quelques d√©tails.

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}

# üßÆ Comparaison de mod√®les d'embedding pour BERTopic
embedding_models <- data.frame(
  Modele = c(
    "paraphrase-distilbert-base-nli-stsb",
    "bert-base-nli-mean-tokens",
    "all-mpnet-base-v2"
  ),
  Taille = c(768, 768, 768),
  Precision_Semantique = c(
    "Bonne pr√©cision s√©mantique",
    "Tr√®s bonne pr√©cision s√©mantique",
    "Excellente pr√©cision s√©mantique"
  )
)
# üìä Affichage du tableau
knitr::kable(embedding_models, caption = "Tableau comparatif de mod√®les d'embedding utilisables avec BERTopic")

```


```{r, warning=FALSE, message=FALSE}
hdbscan_model <- hdbscan$HDBSCAN(
  min_cluster_size = reticulate::r_to_py(3L),
  min_samples = reticulate::r_to_py(1L)
)

#  R√©duction de dimension via UMAP avec seed fix√©e pour reproductibilit√©
umap_model <- umap$UMAP(
  n_neighbors = 15L,
  n_components = 5L,
  min_dist = 0.0,
  metric = "cosine",
  random_state = 42L  # Seed fix√©e ici
)

#  Cr√©ation du mod√®le BERTopic
topic_model <- bertopic$BERTopic(
  language = "french",
  embedding_model = embedding_model,
  hdbscan_model = hdbscan_model,
  umap_model = umap_model
)

```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE, echo=TRUE}

# Pr√©paration des donn√©es

docs <- Texte_JI_filtered$Texte
ids <- Texte_JI_filtered$id  # on garde l'id associ√© √† chaque texte

result <- topic_model$fit_transform(docs)


# Extraction des r√©sultats
topics <- result[[1]]
probs <- result[[2]]

```



```{r}
topics
probs

```


# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#  Reconstruction du data.frame avec id + texte + classe
base_categorisee <- data.frame(
  id = ids,
  texte = docs,
  classe = topics,
  proba = probs
)

# Affichage des infos sur les th√®mes trouv√©s
topic_info <- topic_model$get_topic_info()
```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
head(topic_info)
```


# \textcolor{purple}{\textbf{\textit{5. Cat√©gorisation}}}


Dans cette section, nous tentons de cat√©goriser les textes en se basant sur les diff'rents th√®mes g√©n√©r√©s par le mod√®le.

```{r, warning=FALSE, message=FALSE, echo=TRUE}
topic_info$label <- c(
  "C√©l√©bration et partage des cultures nationales",               # Topic 0
  "Aspect technique et organisation",                             # Topic 1
  "Mieux am√©nager l'espace",                                      # Topic 2
  "Sketchs et prestations",                                       # Topic 3
  "Gestion du timing lors des interventions",                     # Topic 4       
  "Touche cr√©ative et courm√©trages"                                  # Topic 5
)

```

# \textcolor{purple}{\textbf{\textit{}}}
```{r, warning=FALSE, message=FALSE, echo=TRUE}
base_categorisee <- merge(
  base_categorisee, 
  topic_info[, c("Topic", "label")], 
  by.x = "classe", 
  by.y = "Topic", 
  all.x = TRUE
)
base_categorisee <- subset(base_categorisee, select = -c(classe, proba))

doc_vides <- data.frame(
  id = Id_texte_NA
)

# 2. Identifier les noms des autres colonnes (sauf "document")
autres_colonnes <- setdiff(names(base_categorisee), "id")

# 3. Ajouter des NA pour les autres colonnes
doc_vides[autres_colonnes] <- NA
```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE,echo=TRUE}

# 4. Fusionner avec la base existante
textes_by_topic_complet <- rbind(base_categorisee, doc_vides)

# 5. Optionnel: trier par document si n√©cessaire
textes_by_topic_complet <- textes_by_topic_complet[order(textes_by_topic_complet$id), ]

# Joindre les th√®mes dominants avec les tweets originaux

Texte_JI$Texte <- NULL

textes_classified <- Texte_JI %>%
  inner_join(textes_by_topic_complet, by = c("id" = "id"))

```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, warning=FALSE, message=FALSE}
head(textes_classified)
```

# \textcolor{purple}{\textbf{\textit{}}}

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Cr√©ation du tableau bas√© sur l'image
suggestions_table <- data.frame(
  id = 1:10,
  texte = c(
    NA,
    "Donner √† temps le micro aux personnes qui vont chanter.",
    NA,
    "Faire des sketchs courts et bien donner le micro aux acteurs.",
    "Int√©grer un court-m√©trage sur la diversit√© culturelle pour sensibiliser.",
    NA,
    "Faire des sketch courts et donner le micro aux acteurs sur la sc√®ne.",
    NA,
    "Am√©liorer le son pour que tout soit bien audible.",
    NA
  ),
  label = c(
    NA,
    "Gestion du timing lors des interventions",
    NA,
    "Sketchs et prestations",
    "Touche cr√©ative et courm√©trages",
    NA,
    "Sketchs et prestations",
    NA,
    "Aspect technique et organisation",
    NA
  )
)

# üñ®Ô∏è Affichage du tableau joliment format√©
knitr::kable(
  suggestions_table,
  caption = " Suggestions pour am√©liorer l'organisation de la journ√©e d'int√©gration",
  align = "ccl"
)

```


# \textcolor{purple}{\textbf{\textit{CONCLUSION}}}

L‚Äôanalyse textuelle n√©cessite un pr√©traitement rigoureux, mais les outils sont souvent mieux optimis√©s pour l‚Äôanglais, limitant leur efficacit√© pour le fran√ßais. Des m√©thodes comme LDA (bas√©e sur les co-occurrences de mots) ont des limites s√©mantiques, tandis que BERTopic, utilisant des embeddings contextuels (comme BERT), capture mieux le sens des textes. Cependant, l‚Äôanalyse automatique reste imparfaite face √† la diversit√© linguistique et n√©cessite une validation humaine pour des r√©sultats fiables.

# \textcolor{purple}{\textbf{\textit{CONCLUSION}}}

### A retenir 

* Pr√©traitement crucial mais biais√© vers l‚Äôanglais. 

* LDA ‚Üí limite s√©mantique ; 

* BERTopic ‚Üí meilleur sens contextuel. 

* Analyse textuelle = utile mais √† valider par l‚Äôhumain.

