% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fontspec}
\setmainfont{Cambria}
\setsansfont{Franklin Gothic Demi Cond}
\setmonofont{Courier New}
\usepackage[margin=1in]{geometry}
\usepackage{titlesec}
\titleformat{\section}{\Huge\bfseries\color{black}}{\thesection}{1em}{}
\titleformat{\subsection}{\huge\bfseries\color{black}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\LARGE\bfseries\color{black}}{\thesubsubsection}{1em}{}
\usepackage{tocloft}
\renewcommand{\cftsecfont}{\small}
\renewcommand{\cftsubsecfont}{\footnotesize}
\renewcommand{\cftsecpagefont}{\small}
\renewcommand{\cftsubsecpagefont}{\footnotesize}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\begin{titlepage}
    \begin{center}
        \textbf{\LARGE R√âPUBLIQUE DU S√âN√âGAL}\\[0.1cm]
        \includegraphics[width=3cm]{images/Logo1.jpg} \\[0.1cm]  % Ins√®re le chemin de ton logo
        \textbf{\large Un Peuple - Un But - Une Foi}\\[0.2cm]
        
        \textbf{\LARGE Minist√®re de l'√âconomie, du Plan et de la Coop√©ration}\\[0.1cm]
        \includegraphics[width=4cm]{images/Logo2.png} \\[0.1cm] 
        
        \textbf{\large Agence Nationale de la Statistique et de la D√©mographie (ANSD)}\\[0.2cm]
        
        \includegraphics[width=4cm]{images/Logo3.png} \\[0.1cm]  
        
        \textbf{\large √âcole Nationale de la Statistique et de l'Analyse √âconomique Pierre Ndiaye (ENSAE)}\\[0.4cm]
        \includegraphics[width=3cm]{images/Logo4.png} \\[0.1cm]
        
        \textbf{\LARGE PROJET STATISTIQUES SOUS R }\\[0.3cm]
        \textbf{\Huge \color{blue} \textsf{TP10 : Traitement des questions ouvertes avec R : Text mining}}\\[0.2cm]
        \rule{\linewidth}{0.2mm} \\[0.5cm]
        
        \begin{minipage}{0.5\textwidth}
    \begin{flushleft} \large
        \emph{\textsf{R√©dig√© par :}}\\
        \textbf{Mame Balla BOUSSO}\\
        \textbf{Paul BALAFAI}\\
        \textit{El√®ves ing√©nieurs statisticiens √©conomistes}
    \end{flushleft}
\end{minipage}
        \hfill
        \begin{minipage}{0.4\textwidth}
            \begin{flushright} \large
                \emph{\textsf{Sous la supervision de :}} \\
                \textbf{M. Aboubacar HEMA}\\
                \textit{ANALYSTE DE RECHERCHE CHEZ IFPRI }
            \end{flushright}
        \end{minipage}

        \vfill

        {\large \textsf{Ann√©e scolaire : 2024/2025}}\\[0.5cm]
        
    \end{center}
\end{titlepage}

\newpage

\section{Sommaire}\label{sommaire}

\begin{itemize}
\tightlist
\item
  \hyperref[analyse-textuelle-reponses-enquete]{Traitement des questions
  ouvertes avec R}

  \begin{itemize}
  \tightlist
  \item
    \hyperref[importation-nettoyage-donnees]{1. Importation et Nettoyage
    des Donn√©es}
  \item
    \hyperref[exploration-pretraitement-textuel]{2. Exploration et
    Pr√©traitement Textuel}
  \item
    \hyperref[analyse-thematique-lda]{3. Analyse Th√©matique (LDA)}
  \item
    \hyperref[approche-alternative-bertopic]{4. Approche Alternative
    avec BERTopic}
  \item
    \hyperref[visualisation]{5. Cat√©gorisation}
  \item
    \hyperref[conclusion]{CONCLUSION}
  \end{itemize}
\item
  \hyperref[references-webographiques]{R√©f√©rences}
\end{itemize}

\newpage

\section{INTRODUCTION}\label{sec:importation}

Le traitement automatique du langage naturel (TALN) regroupe un ensemble
de techniques permettant d'analyser, de comprendre et de transformer des
textes en donn√©es exploitables. Il existe plusieurs mani√®res d'aborder
le traitement de texte, selon la nature des donn√©es et les objectifs
vis√©s.

La forme la plus courante est l'analyse \textbf{supervis√©e}, o√π chaque
texte est associ√© √† un label pr√©d√©fini. Ces labels peuvent par exemple
repr√©senter des cat√©gories binaires comme 0 ou 1, ou encore des
sentiments comme positif, n√©gatif ou neutre. Dans ce contexte, on
entra√Æne un mod√®le √† apprendre ces correspondances pour ensuite classer
de nouveaux textes.

Cependant, dans de nombreux cas, notamment dans les \textbf{questions
ouvertes d'enqu√™tes}, il n'existe aucune annotation pr√©alable permettant
de guider l'apprentissage. Il devient alors n√©cessaire de structurer les
donn√©es ex nihilo, c'est-√†-dire sans rep√®re pr√©alable, en regroupant les
textes selon leur \textbf{similarit√© s√©mantique}. C'est le domaine de
l'analyse \textbf{non supervis√©e}.

Dans ce cas, une solution consiste √† effectuer une analyse th√©matique,
en divisant le corpus en un \textbf{nombreùêæ} de th√®mes ou sujets, choisi
avec soin. Cette d√©marche vise √† extraire les grandes lignes du contenu
textuel, en r√©v√©lant les motifs r√©currents pr√©sents dans les r√©ponses.
Cela permet d'enrichir l'analyse qualitative des enqu√™tes, m√™me en
l'absence de cat√©gorisation pr√©alable.

Dans cette √©tude, nous nous concentrerons sp√©cifiquement sur cette
approche \textbf{non supervis√©e}. Nous mettrons en ≈ìuvre des techniques
disponibles dans le langage R, notamment gr√¢ce √† des packages comme
\textbf{topicmodels (pour le mod√®le LDA)}, \textbf{tidytext}, et
\textbf{BERTopic} via le package \textbf{reticulate}, pour recourir √†
des mod√®les s√©mantiques plus avanc√©s.

\newpage

\section{1. Importation et Nettoyage des Donn√©es}\label{sec:importation}

\section{Importation et Nettoyage des Donn√©es}\label{sec-importation}

\subsubsection{Chargement des packages}\label{chargement-des-packages}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readxl)       }\CommentTok{\# Pour lire les fichiers Excel}
\FunctionTok{library}\NormalTok{(topicmodels)  }\CommentTok{\# Pour la mod√©lisation th√©matique}
\FunctionTok{library}\NormalTok{(ggplot2)      }\CommentTok{\# Pour les visualisations}
\FunctionTok{library}\NormalTok{(dplyr)        }\CommentTok{\# Pour la manipulation de donn√©es}
\FunctionTok{library}\NormalTok{(tidytext)     }\CommentTok{\# Pour le traitement de texte}
\FunctionTok{library}\NormalTok{(tidyr)        }\CommentTok{\# Pour la gestion des donn√©es}
\FunctionTok{library}\NormalTok{(wordcloud)    }\CommentTok{\# Pour les nuages de mots}
\FunctionTok{library}\NormalTok{(tidyverse)    }\CommentTok{\# Collection de packages pour la science des donn√©es}
\FunctionTok{library}\NormalTok{(tm)           }\CommentTok{\# Pour le text mining}
\FunctionTok{library}\NormalTok{(SnowballC)    }\CommentTok{\# Pour le stemming}
\FunctionTok{library}\NormalTok{(stringr)      }\CommentTok{\# Pour la manipulation de strings}
\end{Highlighting}
\end{Shaded}

\subsection{Importation des donn√©es}\label{importation-des-donnuxe9es}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Enqu√™te\_dopinion\_relative\_√†\_la\_journ√©e\_dint√©gration\_ }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"Data/Enqu√™te\_dopinion\_relative\_√†\_la\_journ√©e\_dint√©gration).xlsx"}\NormalTok{)}
\NormalTok{Texte\_JI }\OtherTok{\textless{}{-}}\NormalTok{ Enqu√™te\_dopinion\_relative\_√†\_la\_journ√©e\_dint√©gration\_}
\FunctionTok{head}\NormalTok{(Enqu√™te\_dopinion\_relative\_√†\_la\_journ√©e\_dint√©gration\_)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##      id `Classe de l‚Äô√©tudiant :` `Nationalit√© de l'√©tudiant` Texte              
##   <dbl> <chr>                    <chr>                       <chr>              
## 1     1 AS2                      Congo                       <NA>               
## 2     2 ISEP1                    Cameroun                    Donner √† temps le ~
## 3     3 AS2                      Congo                       <NA>               
## 4     4 AS1                      S√©n√©gal                     Faire des sketchs ~
## 5     5 AS1                      Cameroun                    Int√©grer un court-~
## 6     6 ISE1 Eco                 S√©n√©gal                     <NA>
\end{verbatim}

\paragraph{V√©rification des
colonnes}\label{vuxe9rification-des-colonnes}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(Texte\_JI)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "id"                        "Classe de l‚Äô√©tudiant :"   
## [3] "Nationalit√© de l'√©tudiant" "Texte"
\end{verbatim}

\subsubsection{Identification des textes
vides}\label{identification-des-textes-vides}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_id\_empty }\OtherTok{\textless{}{-}}\NormalTok{ Texte\_JI }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{nb\_mots =} \FunctionTok{sum}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Texte))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(nb\_mots }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(id)}

\NormalTok{Texte\_JI\_filtered }\OtherTok{\textless{}{-}}\NormalTok{ Texte\_JI }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(id }\SpecialCharTok{\%in\%}\NormalTok{ text\_id\_empty))}

\FunctionTok{head}\NormalTok{(Texte\_JI\_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##      id `Classe de l‚Äô√©tudiant :` `Nationalit√© de l'√©tudiant` Texte              
##   <dbl> <chr>                    <chr>                       <chr>              
## 1     2 ISEP1                    Cameroun                    Donner √† temps le ~
## 2     4 AS1                      S√©n√©gal                     Faire des sketchs ~
## 3     5 AS1                      Cameroun                    Int√©grer un court-~
## 4     7 ISE2                     Cameroun                    Faire des sketch c~
## 5     9 ISEP2                    Cameroun                    Am√©liorer le son p~
## 6    11 ISE2                     Togo                        Une meilleure sono~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(Texte\_JI\_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 45
\end{verbatim}

On remarque que le nombre de ligne a diminu√© passant de 128 √† 45.
Seulement 45 lignes contiennent des textes.

\subsection{Nettoyage des textes}\label{nettoyage-des-textes}

On cr√©e une fonction pour traiter les textes afin de faciliter leur
analyse

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clean\_text }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(text) \{}
\NormalTok{  text }\OtherTok{\textless{}{-}} \FunctionTok{tolower}\NormalTok{(text)           }\CommentTok{\# Conversion en minuscules}
\NormalTok{  text }\OtherTok{\textless{}{-}} \FunctionTok{removePunctuation}\NormalTok{(text) }\CommentTok{\# Suppression de la ponctuation}
\NormalTok{  text }\OtherTok{\textless{}{-}} \FunctionTok{removeNumbers}\NormalTok{(text)     }\CommentTok{\# Suppression des chiffres}
\NormalTok{  text }\OtherTok{\textless{}{-}} \FunctionTok{stripWhitespace}\NormalTok{(text)   }\CommentTok{\# Suppression des espaces superflus}
  \FunctionTok{return}\NormalTok{(text)}
\NormalTok{\}}

\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ Texte\_JI\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Texte\_corrige =} \FunctionTok{sapply}\NormalTok{(Texte, clean\_text))}
\end{Highlighting}
\end{Shaded}

\section{2. Exploration et Pr√©traitement des textes}\label{sec:importation}

Dans le processus de pr√©traitement des donn√©es, on va tokeniser la base
de donn√©es pour analyser non pas les textes, mais les mots directement.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tokenized\_textes }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(id, Texte\_corrige) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{input =} \StringTok{\textquotesingle{}Texte\_corrige\textquotesingle{}}\NormalTok{, }\AttributeTok{output =} \StringTok{\textquotesingle{}word\textquotesingle{}}\NormalTok{)}


\CommentTok{\# Visualisation Tokenisation }

\NormalTok{tokenized\_textes }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{count =}\NormalTok{ n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(count }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{reorder}\NormalTok{(word, count)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ count, }\AttributeTok{y =}\NormalTok{ word)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{()  }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Les mots apparaissant plus de 5 fois"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Nombre total de lignes apr√®s tokenisation}
\FunctionTok{nrow}\NormalTok{(tokenized\_textes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 511
\end{verbatim}

Comme nous pouvons le voir sur le graphique ci-dessus, de nombreux mots
pr√©sents n'apportent aucune r√©elle valeur √† notre analyse. Des mots
comme \textbf{de}, \textbf{pour}, \textbf{les}, \textbf{le}, \textbf{la}
sont ce qu'on appelle des mots vides (stop words).

Nous allons supprimer ces mots en utilisant la commande
\emph{anti\_join(stop\_words)}.

\subsubsection{Charger les stop words en
fran√ßais}\label{charger-les-stop-words-en-franuxe7ais}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stop\_words\_fr }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{word =} \FunctionTok{stopwords}\NormalTok{(}\StringTok{"fr"}\NormalTok{)) }
\FunctionTok{head}\NormalTok{(stop\_words\_fr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 1
##   word 
##   <chr>
## 1 au   
## 2 aux  
## 3 avec 
## 4 ce   
## 5 ces  
## 6 dans
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Stop\_texte }\OtherTok{\textless{}{-}}\NormalTok{ tokenized\_textes }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{anti\_join}\NormalTok{(stop\_words\_fr, }\AttributeTok{by =} \StringTok{"word"}\NormalTok{)}

\FunctionTok{nrow}\NormalTok{(Stop\_texte)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 317
\end{verbatim}

On voit bien que le nombre de mots diminue suite √† la supression des
stop word

Comme nous pouvons le voir sur le graphique ci-dessous, il reste moins
de mots, mais ils sont beaucoup plus pertinents pour l'analyse.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(stop\_words\_fr) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# trouve l√† o√π les textes rencontrent des stop words, et les supprime}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{count =}\NormalTok{ n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(count }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{reorder}\NormalTok{(word, count)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ count, }\AttributeTok{y =}\NormalTok{ word)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Les mots apparaissant plus de 5 fois"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-9-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggwordcloud) }\CommentTok{\# Une autre mani√®re de visualiser}

\NormalTok{Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(stop\_words\_fr) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ word, }\AttributeTok{size =}\NormalTok{ n, }\AttributeTok{color =}\NormalTok{ n)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_text\_wordcloud}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_size\_area}\NormalTok{(}\AttributeTok{max\_size =} \DecValTok{15}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-10-1.pdf}

On peut √©galement voir ci-dessous, la fr√©quence standard des termes (TF)
pour tous les mots

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{count =}\NormalTok{ n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total=}\FunctionTok{sum}\NormalTok{(count))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tf=}\NormalTok{count}\SpecialCharTok{/}\NormalTok{total) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   word   count total     tf
##   <chr>  <int> <int>  <dbl>
## 1 plus      10   317 0.0315
## 2 temps      7   317 0.0221
## 3 bien       6   317 0.0189
## 4 pays       6   317 0.0189
## 5 donner     5   317 0.0158
## 6 faire      5   317 0.0158
\end{verbatim}

L'application des stop word diminue le nombre de mots. Ceci le montre

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 215
\end{verbatim}

\subsubsection{Racinisation}\label{racinisation}

En racinisant, les mots \emph{cultures} et \emph{culture} par exemple se
r√©duisent en \emph{culture}. Voil√† pourquoi le nombre total de mots
diminue comme le r√©sultat de cette commande l'illustre :

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stemming}
\NormalTok{Stop\_texte }\OtherTok{=}\NormalTok{ Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word))}

\CommentTok{\# unique count of words after stemming}
\NormalTok{Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(stem, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 201
\end{verbatim}

Fr√©quence des mots avant le stemming

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fr√©quence des mots avant le stemming}
\NormalTok{Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 2
##    word          n
##    <chr>     <int>
##  1 plus         10
##  2 temps         7
##  3 bien          6
##  4 pays          6
##  5 donner        5
##  6 faire         5
##  7 organiser     5
##  8 tout          5
##  9 chaque        4
## 10 culture       4
\end{verbatim}

Fr√©quence des mots apr√®s le stmming

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fr√©quence des mots apr√®s le stemming}
\NormalTok{Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(stem) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 2
##    stem         n
##    <chr>    <int>
##  1 plu         10
##  2 cultur       8
##  3 temp         7
##  4 bien         6
##  5 organis      6
##  6 pai          6
##  7 culturel     5
##  8 donner       5
##  9 fair         5
## 10 journ√©       5
\end{verbatim}

On remarque que le stemming ne semble pas respecter la logique pour
certains mots. En effet, la racinisation supprime les lettres \emph{s} √†
la fin des mots comme \emph{plus} et \emph{temps}. Egalement le mot
\emph{paix} est r√©duit √† \emph{pai}. Cela constitut une limite majeure
quant √† la racinisation en langue fran√ßaise. C'est pourquoi dans ce qui
suit, nous ferons fi de cette √©tape du pr√©traitement en utilisanat
d√©sormais seulement mes textes issus de l'application des stop word.

\subsubsection{les analyse TF-IDF}\label{les-analyse-tf-idf}

Ci-dessous, nous voyons l'int√©gralit√© du tableau TF-IDF. Ce qui nous
int√©resse le plus, c'est la colonne tf\_idf, car elle nous donne le
classement pond√©r√© ou l'importance des mots dans notre texte.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{texte\_tf\_idf }\OtherTok{\textless{}{-}}\NormalTok{ Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, id, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{count =}\NormalTok{ n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_tf\_idf}\NormalTok{(word, id, count)}
\FunctionTok{head}\NormalTok{(texte\_tf\_idf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
##   word         id count    tf   idf tf_idf
##   <chr>     <dbl> <int> <dbl> <dbl>  <dbl>
## 1 bien         65     2 0.25   2.20  0.549
## 2 cultures    117     2 0.105  2.71  0.285
## 3 dominante   102     2 0.25   3.81  0.952
## 4 pays         75     2 0.143  2.20  0.314
## 5 tout         32     2 0.222  2.42  0.538
## 6 a            39     1 0.2    3.81  0.761
\end{verbatim}

Les simples d√©comptes de fr√©quences de mots peuvent √™tre trompeurs et
peu utiles pour bien comprendre nos donn√©es. Il est en fait int√©ressant
de voir les mots les plus fr√©quents dans chaque texte.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{texte\_tf\_idf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(word, id, tf\_idf, count) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{order\_by =}\NormalTok{ count, }\AttributeTok{n =} \DecValTok{6}\NormalTok{, }\AttributeTok{with\_ties=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\#takes top 5 words from each text}
  \FunctionTok{filter}\NormalTok{(id }\SpecialCharTok{\textless{}} \DecValTok{6}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\#just look at 5 textes}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ word)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_text\_wordcloud}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(}\AttributeTok{rows =} \FunctionTok{vars}\NormalTok{(id))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-17-1.pdf}

On s'est limit√© au cinq premiers textes. Mais les textes correspondant
aux identifiants 1 et 3 sont des NA et donc ont √©t√© isol√©s.

Par ailleurs le r√©sultat qui suit montre aussi qu'il ne faut pas se
limiter √† un simple d√©nombrement des textes mais √† leur fr√©quence.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{texte\_tf\_idf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(word, id, tf\_idf) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{order\_by =}\NormalTok{ tf\_idf,}\AttributeTok{n =} \DecValTok{6}\NormalTok{, }\AttributeTok{with\_ties=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\#takes top 5 words from each text}
  \FunctionTok{filter}\NormalTok{(id }\SpecialCharTok{\textless{}} \DecValTok{6}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\#just look at 5 texts}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ word)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_text\_wordcloud}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(}\AttributeTok{rows =} \FunctionTok{vars}\NormalTok{(id))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-18-1.pdf}

\subsubsection{Relations entre les mots}\label{relations-entre-les-mots}

Jusqu'√† pr√©sent, nous avons seulement examin√© les mots individuellement.
Mais que faire si nous voulons conna√Ætre les relations entre les mots
dans un texte ? Cela peut √™tre accompli gr√¢ce aux n-grammes, o√π n est un
nombre. Auparavant, nous avions effectu√© une tokenisation mot par mot,
mais nous pouvons aussi tokeniser par groupes de n mots. Cr√©ons
maintenant des bigrams (groupes de deux mots) √† partir de tous les
textes, puis comptons-les et trions-les.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{textes\_bigram }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(id, Texte\_corrige) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(bigram, Texte\_corrige, }\AttributeTok{token =} \StringTok{\textquotesingle{}ngrams\textquotesingle{}}\NormalTok{, }\AttributeTok{n =} \DecValTok{2}\NormalTok{) }
\FunctionTok{head}\NormalTok{(textes\_bigram)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##      id bigram       
##   <dbl> <chr>        
## 1     2 donner √†     
## 2     2 √† temps      
## 3     2 temps le     
## 4     2 le micro     
## 5     2 micro aux    
## 6     2 aux personnes
\end{verbatim}

Comme vous pouvez le voir dans le dataframe ci-dessus, certains
bigrammes contiennent des mots vides (stop words) qui n'apportent pas
beaucoup de valeur. Supprimons ces mots vides. Pour cela, nous allons
d'abord s√©parer la colonne des bigrammes en deux colonnes distinctes
nomm√©es `word1' et `word2'. Ensuite, nous utiliserons deux fonctions de
filtre pour supprimer les mots vides.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{textes\_bigram }\OtherTok{\textless{}{-}}\NormalTok{ textes\_bigram }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\CommentTok{\#separates on whitespace}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word1 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words\_fr}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word2 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words\_fr}\SpecialCharTok{$}\NormalTok{word)}

\FunctionTok{head}\NormalTok{(textes\_bigram)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##      id word1     word2     
##   <dbl> <chr>     <chr>     
## 1     2 vont      chanter   
## 2     4 sketchs   courts    
## 3     4 bien      donner    
## 4     4 acteurs   qu‚Äôon     
## 5     4 qu‚Äôon     puisse    
## 6     5 diversit√© culturelle
\end{verbatim}

On peut maintenant compter les bigram et voir le r√©sultat

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bigram\_counts }\OtherTok{\textless{}{-}}\NormalTok{ textes\_bigram }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word1, word2, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(bigram\_counts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   word1       word2        n
##   <chr>       <chr>    <int>
## 1 chaque      pays         3
## 2 diff√©rentes cultures     2
## 3 donner      plus         2
## 4 faut        r√©duire      2
## 5 plus        grande       2
## 6 soir√©e      dansante     2
\end{verbatim}

Comme pr√©c√©demment, on peut aussi cr√©er une mesure TF-IDF avec des
n-grammes. Faisons-le maintenant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(id, Texte\_corrige) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(bigram, Texte\_corrige, }\AttributeTok{token =} \StringTok{\textquotesingle{}ngrams\textquotesingle{}}\NormalTok{, }\AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(id, bigram) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_tf\_idf}\NormalTok{(bigram, id, n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(id, }\FunctionTok{desc}\NormalTok{(tf\_idf)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
## # Groups:   id [1]
##      id bigram            n    tf   idf tf_idf
##   <dbl> <chr>         <int> <dbl> <dbl>  <dbl>
## 1     2 aux personnes     1 0.111  3.81  0.423
## 2     2 donner √†          1 0.111  3.81  0.423
## 3     2 temps le          1 0.111  3.81  0.423
## 4     2 vont chanter      1 0.111  3.81  0.423
## 5     2 √† temps           1 0.111  3.81  0.423
## 6     2 personnes qui     1 0.111  3.11  0.346
\end{verbatim}

Comme on peut le voir ci-dessus, beaucoup de valeurs TF-IDF sont
identiques. Cela est en partie d√ª √† la petite taille des textes. Jetons
maintenant un coup d'≈ìil aux relations entre les mots dans tous les
textes, en utilisant un graphe en r√©seau.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{\textquotesingle{}igraph\textquotesingle{}}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{\textquotesingle{}ggraph\textquotesingle{}}\NormalTok{)}
\NormalTok{bi\_graph }\OtherTok{\textless{}{-}}\NormalTok{ bigram\_counts }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{()}

\FunctionTok{ggraph}\NormalTok{(bi\_graph, }\AttributeTok{layout =} \StringTok{"fr"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name), }\AttributeTok{vjust =} \DecValTok{1}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-23-1.pdf}

Comme on peut le voir ci-dessus, de nombreux noms et d'autres
informations ont √©t√© extraits des donn√©es.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{texte\_trigram }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(id, Texte\_corrige) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(trigram, Texte\_corrige, }\AttributeTok{token =} \StringTok{\textquotesingle{}ngrams\textquotesingle{}}\NormalTok{, }\AttributeTok{n =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(trigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{, }\StringTok{"word3"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\#separates on whitespace}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word1 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words\_fr}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word2 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words\_fr}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word3 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words\_fr}\SpecialCharTok{$}\NormalTok{word)}

\FunctionTok{head}\NormalTok{(texte\_trigram)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##      id word1        word2          word3          
##   <dbl> <chr>        <chr>          <chr>          
## 1     4 acteurs      qu‚Äôon          puisse         
## 2     7 sc√®ne        car            habituellement 
## 3    11 meilleure    sonorisation   c√¥t√©           
## 4    11 sonorisation c√¥t√©           technique      
## 5    14 dautres      journ√©es       continuellement
## 6    14 favoriser    lapprentissage culturel
\end{verbatim}

On peut aussi compter les trigram et voir le r√©sultat

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trigram\_counts }\OtherTok{\textless{}{-}}\NormalTok{ texte\_trigram }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word1, word2, word3, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(trigram\_counts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   word1    word2     word3            n
##   <chr>    <chr>     <chr>        <int>
## 1 acteurs  qu‚Äôon     puisse           1
## 2 ajouter  plus      d‚Äôactivit√©s      1
## 3 am√©nager plus      despace          1
## 4 bien     organiser l‚Äô√©v√©nement      1
## 5 car      c‚Äôest     souvent          1
## 6 certains modules   statistiques     1
\end{verbatim}

Comme pr√©c√©demment, on peut aussi cr√©er une mesure TF-IDF avec des
trigrammes. Faisons-le maintenant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(id, Texte\_corrige) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(trigram, Texte\_corrige, }\AttributeTok{token =} \StringTok{\textquotesingle{}ngrams\textquotesingle{}}\NormalTok{, }\AttributeTok{n =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(id, trigram) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_tf\_idf}\NormalTok{(trigram, id, n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(id, }\FunctionTok{desc}\NormalTok{(tf\_idf)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
## # Groups:   id [1]
##      id trigram                 n    tf   idf tf_idf
##   <dbl> <chr>               <int> <dbl> <dbl>  <dbl>
## 1     2 aux personnes qui       1 0.125  3.81  0.476
## 2     2 donner √† temps          1 0.125  3.81  0.476
## 3     2 micro aux personnes     1 0.125  3.81  0.476
## 4     2 personnes qui vont      1 0.125  3.81  0.476
## 5     2 qui vont chanter        1 0.125  3.81  0.476
## 6     2 temps le micro          1 0.125  3.81  0.476
\end{verbatim}

Comme on peut le voir ci-dessus, beaucoup de valeurs TF-IDF sont
identiques. Cela est en partie d√ª √† la petite taille des textes comme
remarqu√© dans le cas bigram. Jetons maintenant un coup d'≈ìil aux
relations entre les mots dans l'ensemble des textes, en utilisant un
graphe en r√©seau.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tri\_graph }\OtherTok{\textless{}{-}}\NormalTok{ trigram\_counts }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Ici, on garde TOUS les trigrammes pr√©sents au moins une fois}
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{()}

\FunctionTok{ggraph}\NormalTok{(tri\_graph, }\AttributeTok{layout =} \StringTok{"fr"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name), }\AttributeTok{vjust =} \DecValTok{1}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-27-1.pdf}

Normalement, on mettrait n \textgreater{} 1 ou n \textgreater{} 2 pour
filtrer les trigrammes peu fr√©quents, mais dans notre cas, les textets
sont tr√®s courts, donc les trigrammes se r√©p√®tent tr√®s peu.

*R√©sultat : quasiment aucun trigramme n'appara√Æt plus d'une fois. Du
coup, si on filtre avec n \textgreater{} 1 ou n \textgreater{} 2 ‚Üí le
graphe devient vide (aucun trigramme √† afficher).

En mettant n \textgreater{} 0, on garde tous les trigrammes possibles,
m√™me ceux pr√©sents une seule fois. Cela permet d'obtenir un graphe, m√™me
si les connexions sont faibles (juste 1 apparition).

Dans notre base de donn√©es, le champ contenant les suggestions n'est pas
obligatoire, ce qui signifie que plusieurs enregistrements pr√©sentent
des valeurs manquantes (NA). Lors de l'application initiale du mod√®le
LDA sur l'ensemble de la base, nous avons constat√© que certains textes
vides √©taient malgr√© tout class√©s dans une cat√©gorie, simplement parce
qu'ils √©taient pr√©sents dans les donn√©es en entr√©e. En d'autres termes,
le mod√®le attribuait un sujet √† un champ vide, ce qui n'a pas de sens et
fausse l'interpr√©tation : dans la nouvelle variable contenant les
cat√©gories issues du LDA, on retrouvait ainsi des lignes avec un texte
vide associ√© √† une th√©matique, comme si le mod√®le avait `cat√©goris√© du
vide'.

Pour √©viter ce biais, nous avons adopt√© une nouvelle approche plus
rigoureuse. Nous avons d'abord isol√© les textes non vides, c'est-√†-dire
les enregistrements contenant effectivement une suggestion. Le mod√®le
LDA a donc √©t√© appliqu√© uniquement sur cette sous-base, ce qui garantit
que chaque cat√©gorisation repose sur un contenu textuel r√©el.

En parall√®le, nous avons soigneusement conserv√© les identifiants (IDs)
des textes vides, afin de pouvoir les r√©int√©grer dans la base compl√®te
apr√®s classification. Cela permet de reconstituer une base coh√©rente, o√π
:

\begin{itemize}
\item
  -les textes contenant une suggestion sont associ√©s √† une cat√©gorie
  issue du LDA,
\item
  -les textes vides conservent leur place, avec √©ventuellement une
  √©tiquette neutre comme `Non renseign√©' ou NA dans la variable de
  cat√©gorie.
\end{itemize}

Cette m√©thode permet ainsi de respecter la structure initiale de la
base, d'√©viter des classifications erron√©es sur des donn√©es absentes, et
de garantir une analyse fiable et interpr√©table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Extraire les ID avant pr√©traitement (Texte\_JI)}
\NormalTok{Id\_initial\_texte }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(Texte\_JI}\SpecialCharTok{$}\NormalTok{id)}
\FunctionTok{length}\NormalTok{(Id\_initial\_texte)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 128
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2. Extraire les ID apr√®s pr√©traitement (Stop\_texte)}
\NormalTok{Id\_final\_texte }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(Stop\_texte}\SpecialCharTok{$}\NormalTok{id)}
\FunctionTok{length}\NormalTok{(Id\_final\_texte)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 45
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#. Identifier les tweets manquants (pr√©sents avant, absents apr√®s)}
\NormalTok{Id\_texte\_NA }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(Id\_initial\_texte, Id\_final\_texte)}
\FunctionTok{length}\NormalTok{(Id\_texte\_NA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 83
\end{verbatim}

\section{3. Analyse Th√©matique (LDA)}\label{sec:importation}

Il est courant d'avoir une collection de documents, comme des articles
de presse ou des publications sur les r√©seaux sociaux, que l'on souhaite
diviser en th√®mes. Autrement dit, on veut savoir quel est le sujet
principal dans chaque document. Cela peut se faire gr√¢ce √† une technique
appel√©e mod√©lisation th√©matique (topic modeling). Ici, nous allons
explorer la mod√©lisation th√©matique √† travers la m√©thode LDA (Latent
Dirichlet Allocation).

LDA repose sur deux grands principes : Chaque document est un m√©lange de
plusieurs sujets

Chaque sujet est un m√©lange de mots

Un exemple classique serait de supposer qu'il existe deux grands sujets
dans les actualit√©s : la politique et le divertissement. Le sujet
politique contiendra des mots comme √©lu, gouvernement,

Tandis que le sujet divertissement contiendra des mots comme film,
acteur. Mais certains mots peuvent appara√Ætre dans les deux, comme prix
ou budget.

LDA va identifier : les m√©langes de mots qui composent chaque sujet, et
les m√©langes de sujets qui composent chaque document. Voyons cela √†
travers un exemple : On commence par cr√©er notre mod√®le LDA. La fonction
LDA() n√©cessite en entr√©e une matrice document-terme
(DocumentTermMatrix), que l'on peut cr√©er √† partir de notre base d√©j√†
pr√©trait√©e que nous avons g√©n√©r√© pr√©c√©demment.

\subsubsection{Cr√©ation d'une matrice
document-th√®me}\label{cruxe9ation-dune-matrice-document-thuxe8me}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cr√©ation d\textquotesingle{}une matrice document{-}th√®me pour LDA}
\NormalTok{df\_dtm }\OtherTok{\textless{}{-}}\NormalTok{ Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(id, word) }\SpecialCharTok{\%\textgreater{}\%}              
  \FunctionTok{cast\_dtm}\NormalTok{(id, word, n)  }
\end{Highlighting}
\end{Shaded}

\subsection{Choix du nombre k de
th√®mes}\label{choix-du-nombre-k-de-thuxe8mes}

Dans le cadre de la mod√©lisation th√©matique avec LDA (Latent Dirichlet
Allocation), un des √©l√©ments cl√©s du param√©trage est le choix du nombre
de th√®mes (K). Ce param√®tre n'est pas d√©termin√© automatiquement par le
mod√®le ; il doit √™tre choisi par l'utilisateur, en fonction des donn√©es
et des objectifs de l'analyse. Or, le nombre de th√®mes a un impact
direct sur la qualit√© et la lisibilit√© du mod√®le :

Un K trop petit risque de regrouper des th√©matiques tr√®s diff√©rentes
dans un m√™me sujet, rendant le r√©sultat peu pr√©cis.

Un K trop grand peut sur-segmenter les donn√©es, en produisant des th√®mes
trop sp√©cifiques ou redondants, souvent difficiles √† interpr√©ter.

C'est pourquoi il est important de trouver un √©quilibre, c'est-√†-dire un
K optimal qui capte suffisamment de vari√©t√© sans trop complexifier le
mod√®le.

Afin de d√©terminer lenombre K optimal de th√®mes, on utilisons la
perplexit√© du mod√®le pour plusieurs valeurs de K La perplexit√© est une
mesure standard issue de la mod√©lisation probabiliste, souvent utilis√©e
pour √©valuer les mod√®les de langage. Dans le contexte de LDA, elle
mesure dans quelle mesure le mod√®le `explique' les donn√©es textuelles

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k\_values }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{perplexities }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(k\_values, }\ControlFlowTok{function}\NormalTok{(k) \{}
\NormalTok{  lda\_model }\OtherTok{\textless{}{-}} \FunctionTok{LDA}\NormalTok{(df\_dtm, }\AttributeTok{k =}\NormalTok{ k, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{seed =} \DecValTok{1234}\NormalTok{))}
  \FunctionTok{perplexity}\NormalTok{(lda\_model)}
\NormalTok{\})}

\CommentTok{\# Afficher le graphique}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{K =}\NormalTok{ k\_values, }\AttributeTok{Perplexity =}\NormalTok{ perplexities), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ K, }\AttributeTok{y =}\NormalTok{ Perplexity)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Choix du K optimal avec la perplexit√©"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Nombre de th√®mes (K)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Perplexit√©"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-30-1.pdf}

Le graphique montre une forte diminution de la perplexit√© entre K = 2 et
K = 7, ce qui indique que chaque th√®me ajout√© dans cette plage apporte
une r√©elle am√©lioration du mod√®le. Ensuite, √† partir de K ‚âà 8, la courbe
commence √† s'aplatir : les gains suppl√©mentaires deviennent de plus en
plus faibles.

Ce comportement sugg√®re qu'√† partir de K = 8, ajouter davantage de
th√®mes n'am√©liore plus significativement la qualit√© du mod√®le, tout en
augmentant sa complexit√©. On peut donc consid√©rer K = 8 comme un bon
compromis, car il permet de capter une diversit√© raisonnable de
th√©matiques sans trop fragmenter les donn√©es.

Cela justifie donc le choix de 8 th√®mes comme valeur optimale dans notre
mod√©lisation LDA.

\paragraph{G√©n√©ralement}\label{guxe9nuxe9ralement}

En th√©orie, la perplexit√© est cens√©e diminuer √† mesure que le nombre de
th√®mes (K) augmente. En effet, un mod√®le avec plus de th√®mes dispose de
plus de `flexibilit√©' pour repr√©senter les textes de mani√®re fine. Cela
se traduit g√©n√©ralement par une meilleure capacit√© √† pr√©dire les mots
observ√©s dans les documents --- donc une perplexit√© plus faible.

Cependant, ce comportement n'est pas garanti dans tous les cas. Il peut
arriver que la perplexit√© stagne voire augmente √† partir d'un certain K,
ou ne suive pas une baisse r√©guli√®re. Ce ph√©nom√®ne peut √™tre li√© √†
plusieurs facteurs, notamment √† la nature des textes analys√©s.

Un cas courant :

Les mots utilis√©s dans les documents peuvent √™tre tr√®s vari√©s m√™me s'ils
expriment des id√©es similaires. Par exemple, des mots comme
\textbf{gouvernement}, \textbf{√âtat}, \textbf{autorit√©s},
\textbf{institution} peuvent tous renvoyer √† la m√™me notion politique,
mais √™tre trait√©s comme des termes distincts par le mod√®le. Cela peut
fragmenter artificiellement les th√®mes, ou faire croire √† une diversit√©
de contenus plus grande qu'en r√©alit√©.

Dans ces situations, la perplexit√© peut ne plus refl√©ter fid√®lement la
\textbf{coh√©rence s√©mantique} des th√®mes. Elle devient donc une mesure
limit√©e, surtout si les textes sont courts, informels ou s'ils
contiennent beaucoup de synonymes ou paraphrases.

\subsubsection{Une solution souvent utilis√©e : Groupage par
th√®me}\label{une-solution-souvent-utilisuxe9e-groupage-par-thuxe8me}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_df2 }\OtherTok{\textless{}{-}}\NormalTok{ Stop\_texte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{text\_semantic =}\NormalTok{ word) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# dupliquer la colonne lemmatis√©e}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{text\_semantic =} \FunctionTok{str\_replace\_all}\NormalTok{(text\_semantic, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b(manger|plat|mets|piment|restaurant)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, }\StringTok{"alimentation"}\NormalTok{),}
    \AttributeTok{text\_semantic =} \FunctionTok{str\_replace\_all}\NormalTok{(text\_semantic, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b(sketch|micro|temps|chanter|court|courtm√©trage|sketcheval|sc√®ne|culture|culturel|lapprentissage|salle|pr√©sentation|apprendre|discours|m√©trage|fun|espace|prestataire|marrant|comique|communaut√©|am√©nager)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, }\StringTok{"animation"}\NormalTok{),}
    \AttributeTok{text\_semantic =} \FunctionTok{str\_replace\_all}\NormalTok{(text\_semantic, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b(audible|sonorisation|technique)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, }\StringTok{"technique"}\NormalTok{),}
    \AttributeTok{text\_semantic =} \FunctionTok{str\_replace\_all}\NormalTok{(text\_semantic, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b(communication|organisation|cr√©ativit√©|organiser)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, }\StringTok{"Organisation"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\subsubsection{Limites : pas trop flexible surtout en cas de grands
volumes de
donn√©es}\label{limites-pas-trop-flexible-surtout-en-cas-de-grands-volumes-de-donnuxe9es}

Dans ce qui suit, nous continuerons directement avec les donn√©es d√©j√†
pr√©trait√©es et visualis√©es sans appliquer un groupage suppl√©mentaire.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda\_model }\OtherTok{\textless{}{-}} \FunctionTok{LDA}\NormalTok{(df\_dtm, }\AttributeTok{k =} \DecValTok{8}\NormalTok{, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{seed =} \DecValTok{1234}\NormalTok{))}

\CommentTok{\# Termes par th√®me}
\NormalTok{terms\_by\_topic }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(lda\_model, }\AttributeTok{matrix =} \StringTok{"beta"}\NormalTok{)}
\NormalTok{terms\_by\_topic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,720 x 3
##    topic term         beta
##    <int> <chr>       <dbl>
##  1     1 chanter 5.02e-154
##  2     2 chanter 4.94e-324
##  3     3 chanter 5.02e-154
##  4     4 chanter 1.88e-154
##  5     5 chanter 1.81e-  2
##  6     6 chanter 3.48e-154
##  7     7 chanter 2.77e-154
##  8     8 chanter 1.88e-154
##  9     1 donner  1.14e-153
## 10     2 donner  4.88e-  2
## # i 1,710 more rows
\end{verbatim}

La colonne beta repr√©sente la probabilit√© qu'un mot donn√© appartienne √†
un th√®me particulier. En d'autres termes, plus la valeur de beta est
√©lev√©e pour un mot dans un th√®me, plus ce mot est repr√©sentatif de ce
th√®me

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_terms }\OtherTok{\textless{}{-}}\NormalTok{ terms\_by\_topic }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(topic) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_max}\NormalTok{(beta, }\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{with\_ties =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# Prend exactement 10 termes par th√®me}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(topic, }\SpecialCharTok{{-}}\NormalTok{beta)  }\CommentTok{\# Trie par th√®me et par probabilit√© d√©croissante}

\CommentTok{\# V√©rification du nombre de termes s√©lectionn√©s par th√®me}
\NormalTok{top\_terms }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(topic) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 2
##   topic     n
##   <int> <int>
## 1     1    10
## 2     2    10
## 3     3    10
## 4     4    10
## 5     5    10
## 6     6    10
## 7     7    10
## 8     8    10
\end{verbatim}

1O termes ont √©t√© s√©lectionn√©s par th√®mes. On peut les visualiser
√©galement

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_terms }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{term =} \FunctionTok{reorder\_within}\NormalTok{(term, beta, topic)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(beta, term, }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(topic))) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ topic, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_reordered}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Termes les plus probables par th√®me"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Probabilit√© (beta)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Terme"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-34-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# √âtape 4 : Classification des textes par th√®me}
\NormalTok{textes\_gamma }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(lda\_model, }\AttributeTok{matrix =} \StringTok{"gamma"}\NormalTok{)}
\CommentTok{\# Afficher les textes avec leur th√®me dominant}
\NormalTok{textes\_classified }\OtherTok{\textless{}{-}}\NormalTok{ textes\_gamma }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(document) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_max}\NormalTok{(gamma) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Ici pour chaque texte, on peut voir la probabilit√© qu'il a d'appartenir
√† chacun des th√®mes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Nombre de texte dans chaque th√®me}
\NormalTok{textes\_classified }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(topic)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 2
##   topic     n
##   <int> <int>
## 1     1     3
## 2     2     6
## 3     3     3
## 4     4     7
## 5     5     8
## 6     6     3
## 7     7     5
## 8     8    10
\end{verbatim}

Pour chaque th√®me, on voit le nombre de textes

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualisation des nombres de texte dans chaque th√®me}
\NormalTok{textes\_classified }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\FunctionTok{factor}\NormalTok{(topic), }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(topic))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution des th√®mes dominants"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Th√®me"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Nombre de textes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Texte_mining_files/figure-latex/unnamed-chunk-37-1.pdf}

\subsubsection{Labellisation (tr√®s
subjective)}\label{labellisation-truxe8s-subjective}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Labelliser les th√®mes}

\NormalTok{textes\_gamma }\OtherTok{\textless{}{-}}\NormalTok{ textes\_gamma }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{topic\_label =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"Diversit√© culturelle et activit√©s communes"}\NormalTok{,}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"Performances artistiques et intervention sc√©nique"}\NormalTok{,}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{"Pr√©sentation des cultures par les communaut√©s"}\NormalTok{,}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \StringTok{"Am√©nagement de l\textquotesingle{}espace et la gestion du temps"}\NormalTok{,}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \StringTok{"Organisation"}\NormalTok{,}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{6} \SpecialCharTok{\textasciitilde{}} \StringTok{"Suggestions d\textquotesingle{}am√©lioration"}\NormalTok{,}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{7} \SpecialCharTok{\textasciitilde{}} \StringTok{"Organisation g√©n√©rale et impression globale"}\NormalTok{,}
\NormalTok{    topic }\SpecialCharTok{==} \DecValTok{8} \SpecialCharTok{\textasciitilde{}} \StringTok{"animation"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Autre"}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

\section{4. Approche Alternative avec BERTopic}\label{sec:importation}

BERTopic est un outil puissant de topic modeling (mod√©lisation de
sujets) qui permet d'extraire automatiquement des th√®mes principaux √†
partir de textes non structur√©s. Il se distingue des approches
classiques comme LDA par sa capacit√© √† capturer des relations
s√©mantiques en se basant sur le texte et non des motstokenis√©s.

\subsection{Installation de miniconda et chargement du package
reticulate}\label{installation-de-miniconda-et-chargement-du-package-reticulate}

Pour utiliser les biblioth√®ques Python dans R (comme bertopic,
sentence-transformers, etc.) qui sont n√©cessaire √† notre analyse, on
utilise le package reticulate, qui agit comme un pont entre R et Python.
Afin d'assurer que tout fonctionne dans un environnement propre et
contr√¥l√©, nous allons installer Miniconda, une version l√©g√®re de Conda,
qui sert √† g√©rer les environnements Python.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}

\CommentTok{\# Voir l\textquotesingle{}environnement actif de reticulate}

\FunctionTok{py\_config}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in python_config_impl(python) : 
##   Error 103 occurred running C:/Users/lenovo/Documents/.virtualenvs/r-reticulate/Scripts/python.exe:
\end{verbatim}

\begin{verbatim}
## python:         C:/Users/lenovo/AppData/Local/R/cache/R/reticulate/uv/cache/archive-v0/6h8sIL35_M-JaWAg2n46t/Scripts/python.exe
## libpython:      C:/Users/lenovo/AppData/Local/R/cache/R/reticulate/uv/python/cpython-3.11.12-windows-x86_64-none/python311.dll
## pythonhome:     C:/Users/lenovo/AppData/Local/R/cache/R/reticulate/uv/cache/archive-v0/6h8sIL35_M-JaWAg2n46t
## virtualenv:     C:/Users/lenovo/AppData/Local/R/cache/R/reticulate/uv/cache/archive-v0/6h8sIL35_M-JaWAg2n46t/Scripts/activate_this.py
## version:        3.11.12 (main, Apr  9 2025, 04:03:34) [MSC v.1943 64 bit (AMD64)]
## Architecture:   64bit
## numpy:          C:/Users/lenovo/AppData/Local/R/cache/R/reticulate/uv/cache/archive-v0/6h8sIL35_M-JaWAg2n46t/Lib/site-packages/numpy
## numpy_version:  2.2.4
## 
## NOTE: Python version was forced by py_require()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Installation des packages n√©cessaires dans l‚Äôenvironnement actif de reticulate}
\NormalTok{reticulate}\SpecialCharTok{::}\FunctionTok{py\_install}\NormalTok{(}
  \AttributeTok{packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"sentence{-}transformers"}\NormalTok{, }\StringTok{"hdbscan"}\NormalTok{, }\StringTok{"umap{-}learn"}\NormalTok{, }\StringTok{"bertopic"}\NormalTok{),}
  \AttributeTok{pip =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Importation des modules
Python}\label{importation-des-modules-python}

Chaque module que nous importons est li√© √† une fonctionnalit√© cl√© :

\begin{itemize}
\tightlist
\item
  sentence\_transformers : gestion des mod√®les d'embedding de texte
\item
  hdbscan : algorithme de clustering utilis√© par BERTopic
\item
  bertopic : la librairie principale pour la mod√©lisation de sujets
\item
  umap : utilis√© pour projeter les embeddings dans un espace de plus
  faible dimension
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentence\_transformers }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"sentence\_transformers"}\NormalTok{)}
\NormalTok{hdbscan }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"hdbscan"}\NormalTok{)}
\NormalTok{bertopic }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"bertopic"}\NormalTok{)}
\NormalTok{umap }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"umap"}\NormalTok{)  }\CommentTok{\# important pour fixer le random\_state}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# üî§ Mod√®le d\textquotesingle{}embedding (changeable par d\textquotesingle{}autres plus bas)}
\NormalTok{embedding\_model }\OtherTok{\textless{}{-}}\NormalTok{ sentence\_transformers}\SpecialCharTok{$}\FunctionTok{SentenceTransformer}\NormalTok{(}\StringTok{"paraphrase{-}MiniLM{-}L6{-}v2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ici on utilise `paraphrase-MiniLM-L6-v2', un mod√®le rapide et efficace.
Mais il existe d'autres vari√©t√©s plus puissante mais qui sont plus
robuste. Le tableau qui suit donne quelques d√©tails.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# üßÆ Comparaison de mod√®les d\textquotesingle{}embedding pour BERTopic}
\NormalTok{embedding\_models }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Modele =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"paraphrase{-}distilbert{-}base{-}nli{-}stsb"}\NormalTok{,}
    \StringTok{"bert{-}base{-}nli{-}mean{-}tokens"}\NormalTok{,}
    \StringTok{"all{-}mpnet{-}base{-}v2"}
\NormalTok{  ),}
  \AttributeTok{Taille =} \FunctionTok{c}\NormalTok{(}\DecValTok{768}\NormalTok{, }\DecValTok{768}\NormalTok{, }\DecValTok{768}\NormalTok{),}
  \AttributeTok{Precision\_Semantique =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"Bonne pr√©cision s√©mantique"}\NormalTok{,}
    \StringTok{"Tr√®s bonne pr√©cision s√©mantique"}\NormalTok{,}
    \StringTok{"Excellente pr√©cision s√©mantique"}
\NormalTok{  )}
\NormalTok{)}
\CommentTok{\# üìä Affichage du tableau}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(embedding\_models, }\AttributeTok{caption =} \StringTok{"Tableau comparatif de mod√®les d\textquotesingle{}embedding utilisables avec BERTopic"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4800}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0933}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4267}}@{}}
\caption{Tableau comparatif de mod√®les d'embedding utilisables avec
BERTopic}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Modele
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Taille
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Precision\_Semantique
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Modele
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Taille
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Precision\_Semantique
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
paraphrase-distilbert-base-nli-stsb & 768 & Bonne pr√©cision
s√©mantique \\
bert-base-nli-mean-tokens & 768 & Tr√®s bonne pr√©cision s√©mantique \\
all-mpnet-base-v2 & 768 & Excellente pr√©cision s√©mantique \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hdbscan\_model }\OtherTok{\textless{}{-}}\NormalTok{ hdbscan}\SpecialCharTok{$}\FunctionTok{HDBSCAN}\NormalTok{(}
  \AttributeTok{min\_cluster\_size =}\NormalTok{ reticulate}\SpecialCharTok{::}\FunctionTok{r\_to\_py}\NormalTok{(}\DecValTok{3}\DataTypeTok{L}\NormalTok{),}
  \AttributeTok{min\_samples =}\NormalTok{ reticulate}\SpecialCharTok{::}\FunctionTok{r\_to\_py}\NormalTok{(}\DecValTok{1}\DataTypeTok{L}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# üéØ R√©duction de dimension via UMAP avec seed fix√©e pour reproductibilit√©}
\NormalTok{umap\_model }\OtherTok{\textless{}{-}}\NormalTok{ umap}\SpecialCharTok{$}\FunctionTok{UMAP}\NormalTok{(}
  \AttributeTok{n\_neighbors =} \DecValTok{15}\DataTypeTok{L}\NormalTok{,}
  \AttributeTok{n\_components =} \DecValTok{5}\DataTypeTok{L}\NormalTok{,}
  \AttributeTok{min\_dist =} \FloatTok{0.0}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"cosine"}\NormalTok{,}
  \AttributeTok{random\_state =} \DecValTok{42}\DataTypeTok{L}  \CommentTok{\# ‚úÖ Seed fix√©e ici}
\NormalTok{)}

\CommentTok{\# üìö Cr√©ation du mod√®le BERTopic}
\NormalTok{topic\_model }\OtherTok{\textless{}{-}}\NormalTok{ bertopic}\SpecialCharTok{$}\FunctionTok{BERTopic}\NormalTok{(}
  \AttributeTok{language =} \StringTok{"french"}\NormalTok{,}
  \AttributeTok{embedding\_model =}\NormalTok{ embedding\_model,}
  \AttributeTok{hdbscan\_model =}\NormalTok{ hdbscan\_model,}
  \AttributeTok{umap\_model =}\NormalTok{ umap\_model}
\NormalTok{)}

\CommentTok{\# üì¶ Pr√©paration des donn√©es}

\NormalTok{docs }\OtherTok{\textless{}{-}}\NormalTok{ Texte\_JI\_filtered}\SpecialCharTok{$}\NormalTok{Texte}
\NormalTok{ids }\OtherTok{\textless{}{-}}\NormalTok{ Texte\_JI\_filtered}\SpecialCharTok{$}\NormalTok{id  }\CommentTok{\# on garde l\textquotesingle{}id associ√© √† chaque texte}

\NormalTok{result }\OtherTok{\textless{}{-}}\NormalTok{ topic\_model}\SpecialCharTok{$}\FunctionTok{fit\_transform}\NormalTok{(docs)}


\CommentTok{\# üéØ Extraction des r√©sultats}
\NormalTok{topics }\OtherTok{\textless{}{-}}\NormalTok{ result[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{probs }\OtherTok{\textless{}{-}}\NormalTok{ result[[}\DecValTok{2}\NormalTok{]]}

\NormalTok{topics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 4 3 5 3 1 0 0 0 2 2 1 0 2 2 3 0 5 0 3 4 1 1 1 1 3 3 1 0 0 2 0 4 0 0 0 0 1 2
## [39] 1 5 4 0 1 0 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.7649678 1.0000000 1.0000000 0.7850124 1.0000000 0.8767291 1.0000000
##  [8] 1.0000000 1.0000000 1.0000000 0.9240548 1.0000000 0.8713598 1.0000000
## [15] 1.0000000 1.0000000 1.0000000 0.9533261 1.0000000 1.0000000 1.0000000
## [22] 0.9240548 0.9210614 1.0000000 0.8374228 0.6877206 1.0000000 1.0000000
## [29] 1.0000000 0.8713598 1.0000000 1.0000000 0.9533261 1.0000000 0.8585130
## [36] 1.0000000 1.0000000 0.9615890 0.9210614 1.0000000 1.0000000 1.0000000
## [43] 1.0000000 1.0000000 0.8038874
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# üîÅ Reconstruction du data.frame avec id + texte + classe}
\NormalTok{base\_categorisee }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{id =}\NormalTok{ ids,}
  \AttributeTok{texte =}\NormalTok{ docs,}
  \AttributeTok{classe =}\NormalTok{ topics,}
  \AttributeTok{proba =}\NormalTok{ probs}
\NormalTok{)}

\CommentTok{\# üîç Affichage des infos sur les th√®mes trouv√©s}
\NormalTok{topic\_info }\OtherTok{\textless{}{-}}\NormalTok{ topic\_model}\SpecialCharTok{$}\FunctionTok{get\_topic\_info}\NormalTok{()}
\FunctionTok{head}\NormalTok{(topic\_info)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Topic Count                                Name
## 1     0    15                    0_de_la_pays_les
## 2     1    10                1_bien_pour_soit_son
## 3     2     6             2_tout_une_soir√©e_monde
## 4     3     6        3_sketchs_des_courts_acteurs
## 5     4     5      4_temps_prestations_r√©duire_le
## 6     5     3 5_court_m√©trage_culturelle_int√©grer
##                                                                                              Representation
## 1                                 de, la, pays, les, pr√©senter, culture, organisation, cultures, chaque, en
## 2                                              bien, pour, soit, son, jours, espace, le, est, journ√©e, tout
## 3                                           tout, une, soir√©e, monde, voir, salle, grande, dansante, je, un
## 4                                               sketchs, des, courts, acteurs, les, aux, micro, et, je, pas
## 5                               temps, prestations, r√©duire, le, il, faut, des, chanter, accordez, diminuer
## 6 court, m√©trage, culturelle, int√©grer, cr√©ativit√©, preuve, diversit√©, participants, sensibiliser, culturel
##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Representative_Docs
## 1 Que chaque pays pr√©sente sa culture de telle sorte les personnes qui ne connaissaient pas ce  pays vont le d√©couvrir √† travers la prestation de ses r√©sidents, Encourager la co-organisation pour permettre aux diff√©rentes cultures de nous pr√©senter des activit√©s culturelles en commun. Prolonger la journ√©e d'int√©gration √† d'autres journ√©es (continuellement) pour favoriser l'apprentissage culturel au sein de l'√©cole, Les diff√©rentes communaut√©s doivent s'avantage s'enraciner dans leur cultures et pr√©senter les sp√©cificit√©s de leur cultures afin de susciter la curiousit√© des autres et voulant en savoir davantage m√™me apr√®s la dite journ√©e
## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                       Bien organiser l‚Äô√©v√©nement pour √©viter le d√©sordre et les retards., Am√©liorer le son pour que tout soit bien audible., V√©rifier le son pour que tout soit bien clair et qu‚Äôon entende bien les prestataires
## 3                                                                                                                                                                                                                                                                                                                                                                                                                        Je ne crois pas qu'on. L'air d√©j√† fait mais je trouve que √ßa pourrait √™tre int√©ressant, Utiliser une plus grande salle pour permettre √† tout le monde de tout voir, Trouver une salle plus grande pour que tout le monde puisse bien voir.
## 4                                                                                                                                                                                                                                                                                                                              Je sugg√®re des sketchs comiques qui vont nous permettre de r√©parer les d√©g√¢ts psychologiques caus√©s par le premier semestre, Faire des sketch courts et donner le micro aux acteurs sur la sc√®ne, car habituellement on ne les √©coute pas., Faire des sketchs courts et bien donner le micro aux acteurs, qu‚Äôon puisse les entendre.
## 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                diminuer le temps des discours, Il faut r√©duire le temps des prestations, Il faut r√©duire le temps des prestations
## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Faire preuve de cr√©ativit√©, Court m√©trage culturel, Int√©grer un court-m√©trage sur la diversit√© culturelle pour sensibiliser les participants
\end{verbatim}

\section{5. Cat√©gorisation}\label{sec:importation}

Dans cette section, nous tentons de cat√©goriser les textes en se basant
sur les diff'rents th√®mes g√©n√©r√©s par le mod√®le.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{topic\_info}\SpecialCharTok{$}\NormalTok{label }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \StringTok{"C√©l√©bration et partage des cultures nationales"}\NormalTok{,               }\CommentTok{\# Topic 0}
  \StringTok{"Aspect technique et organisation"}\NormalTok{,                             }\CommentTok{\# Topic 1}
  \StringTok{"Mieux am√©nager l\textquotesingle{}espace"}\NormalTok{,                                      }\CommentTok{\# Topic 2}
  \StringTok{"Sketchs et prestations"}\NormalTok{,                                       }\CommentTok{\# Topic 3}
  \StringTok{"Gestion du timing lors des interventions"}\NormalTok{,                     }\CommentTok{\# Topic 4       }
  \StringTok{"Touche cr√©ative et courm√©trages"}                                  \CommentTok{\# Topic 5}
\NormalTok{)}


\NormalTok{base\_categorisee }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(}
\NormalTok{  base\_categorisee, }
\NormalTok{  topic\_info[, }\FunctionTok{c}\NormalTok{(}\StringTok{"Topic"}\NormalTok{, }\StringTok{"label"}\NormalTok{)], }
  \AttributeTok{by.x =} \StringTok{"classe"}\NormalTok{, }
  \AttributeTok{by.y =} \StringTok{"Topic"}\NormalTok{, }
  \AttributeTok{all.x =} \ConstantTok{TRUE}
\NormalTok{)}


\NormalTok{base\_categorisee }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(base\_categorisee, }\AttributeTok{select =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(classe, proba))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc\_vides }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{id =}\NormalTok{ Id\_texte\_NA}
\NormalTok{)}


\CommentTok{\# 2. Identifier les noms des autres colonnes (sauf "document")}
\NormalTok{autres\_colonnes }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(}\FunctionTok{names}\NormalTok{(base\_categorisee), }\StringTok{"id"}\NormalTok{)}

\CommentTok{\# 3. Ajouter des NA pour les autres colonnes}
\NormalTok{doc\_vides[autres\_colonnes] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\CommentTok{\# 4. Fusionner avec la base existante}
\NormalTok{textes\_by\_topic\_complet }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(base\_categorisee, doc\_vides)}

\CommentTok{\# 5. Optionnel: trier par document si n√©cessaire}
\NormalTok{textes\_by\_topic\_complet }\OtherTok{\textless{}{-}}\NormalTok{ textes\_by\_topic\_complet[}\FunctionTok{order}\NormalTok{(textes\_by\_topic\_complet}\SpecialCharTok{$}\NormalTok{id), ]}

\CommentTok{\# Joindre les th√®mes dominants avec les tweets originaux}

\NormalTok{Texte\_JI}\SpecialCharTok{$}\NormalTok{Texte }\OtherTok{\textless{}{-}} \ConstantTok{NULL}

\NormalTok{textes\_classified }\OtherTok{\textless{}{-}}\NormalTok{ Texte\_JI }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(textes\_by\_topic\_complet, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"id"} \OtherTok{=} \StringTok{"id"}\NormalTok{))}

\FunctionTok{head}\NormalTok{(textes\_classified)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##      id `Classe de l‚Äô√©tudiant :` `Nationalit√© de l'√©tudiant` texte         label
##   <dbl> <chr>                    <chr>                       <chr>         <chr>
## 1     1 AS2                      Congo                       <NA>          <NA> 
## 2     2 ISEP1                    Cameroun                    Donner √† tem~ Gest~
## 3     3 AS2                      Congo                       <NA>          <NA> 
## 4     4 AS1                      S√©n√©gal                     Faire des sk~ Sket~
## 5     5 AS1                      Cameroun                    Int√©grer un ~ Touc~
## 6     6 ISE1 Eco                 S√©n√©gal                     <NA>          <NA>
\end{verbatim}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0336}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6218}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3445}}@{}}
\caption{üí° Suggestions pour am√©liorer l'organisation de la journ√©e
d'int√©gration}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
id
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
texte
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
label
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
id
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
texte
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
label
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & NA & NA \\
2 & Donner √† temps le micro aux personnes qui vont chanter. & Gestion du
timing lors des interventions \\
3 & NA & NA \\
4 & Faire des sketchs courts et bien donner le micro aux acteurs. &
Sketchs et prestations \\
5 & Int√©grer un court-m√©trage sur la diversit√© culturelle pour
sensibiliser. & Touche cr√©ative et courm√©trages \\
6 & NA & NA \\
7 & Faire des sketch courts et donner le micro aux acteurs sur la sc√®ne.
& Sketchs et prestations \\
8 & NA & NA \\
9 & Am√©liorer le son pour que tout soit bien audible. & Aspect technique
et organisation \\
10 & NA & NA \\
\end{longtable}

\newpage
\section{CONCLUSION}\label{sec:importation}

L'analyse des questions ouvertes √† l'aide des techniques de text mining
met en lumi√®re l'importance du \textbf{pr√©traitement} des donn√©es
textuelles. Cette √©tape cruciale permet de nettoyer, normaliser et
structurer le texte afin de le rendre exploitable pour les algorithmes
d'analyse. Cependant, il est important de noter que les outils de
pr√©traitement sont plus adapt√©s et optimis√©s pour \textbf{l'anglais},
notamment en ce qui concerne les listes de \textbf{stop words}, les
outils de \textbf{stemming} ou de \textbf{lemmatisation}. Cela constitue
un frein lorsqu'on travaille sur des textes en fran√ßais ou dans d'autres
langues moins repr√©sent√©es.

Parmi les m√©thodes explor√©es, \textbf{LDA (Latent Dirichlet Allocation)}
permet d'identifier des th√©matiques en se basant sur la fr√©quence des
mots. Toutefois, cette approche pr√©sente des \textbf{limites
importantes} : elle repose uniquement sur la \textbf{co-occurrence de
mots}, sans prendre en compte leur sens r√©el ou leur contexte
s√©mantique. Ainsi, des textes exprimant des id√©es similaires avec des
mots diff√©rents peuvent ne pas √™tre associ√©s au m√™me th√®me, ce qui
r√©duit la pertinence de l'analyse dans certains cas.

C'est dans ce cadre que *BERTopic** se distingue. En s'appuyant sur des
mod√®les d'embeddings comme BERT, il permet de capter la s√©mantique des
phrases. Il devient alors possible de regrouper des textes similaires
m√™me si les mots employ√©s sont diff√©rents. Cette approche offre une
compr√©hension plus fine et plus pertinente des id√©es exprim√©es dans les
donn√©es.

Cela dit, il est essentiel de garder √† l'esprit que le traitement des
textes reste une t√¢che \textbf{complexe} et \textbf{imparfaite}. La
diversit√© des styles, des formulations, des niveaux de langue ou encore
des erreurs d'√©criture rend l'analyse automatique difficile. Les
r√©sultats doivent donc √™tre interpr√©t√©s avec pr√©caution, et id√©alement
compl√©t√©s par une validation \textbf{humaine} pour garantir leur
fiabilit√©.

\section{R√©f√©rences
Bibliographiques}\label{ruxe9fuxe9rences-bibliographiques}

\begin{itemize}
\item
  \href{https://www.innovatiana.com/post/best-datasets-for-text-classification}{Classification}
\item
  \href{https://guides.library.upenn.edu/penntdm/r}{Pr√©traitement}
\item
  \href{https://bookdown.org/tianyuan09/ai4ph2022/tutorial.html}{Pr√©traitement}
\item
  \href{https://ladal.edu.au/tutorials/topic/topic.html?}{Topic
  modeling}
\end{itemize}

\end{document}
