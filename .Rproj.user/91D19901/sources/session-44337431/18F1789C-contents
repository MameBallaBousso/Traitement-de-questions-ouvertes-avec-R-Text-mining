# Charger les bibliothèques nécessaires

library(readxl)
library(topicmodels)
library(ggplot2)
library(dplyr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(tidyverse)
library(tm)
library(SnowballC)
library(stringr)
library(udpipe) # Pour la lemmatisation



Enquête_dopinion_relative_à_la_journée_dintégration_ <- read_excel("Data/Enquête_dopinion_relative_à_la_journée_dintégration).xlsx")
View(Enquête_dopinion_relative_à_la_journée_dintégration_)
Texte_JI <- Enquête_dopinion_relative_à_la_journée_dintégration_

colnames(Texte_JI)

# Isoler les lignes vides pour textes
text_id_empty <- Texte_JI %>%
  group_by(id) %>%
  summarise(nb_mots = sum(!is.na(Texte))) %>%
  filter(nb_mots == 0) %>%
  pull(id)

# Garde uniquement les documents non vides pour le LDA
Texte_JI_filtered <- Texte_JI %>% 
  filter(!(id %in% text_id_empty))

# Afficher les premières lignes
head(Texte_JI_filtered)

# Nombre total de lignes après filtrage
nrow(Texte_JI_filtered)

# Fonction de nettoyage du texte
clean_text <- function(text) {
  text <- tolower(text)           # Minuscule
  text <- removePunctuation(text) # Enlever la ponctuation
  text <- removeNumbers(text)     # Enlever les chiffres
  text <- stripWhitespace(text)   # Supprimer les espaces en trop
  return(text)
}

# Ajouter une nouvelle colonne "Texte_corrige"
data <- Texte_JI_filtered %>%
  mutate(Texte_corrige = sapply(Texte, clean_text))
# Vérifier l'ajout de la colonne corrigée

glimpse(data)

tokenized_textes <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(input = 'Texte_corrige', output = 'word')


# Visualisation Tokenisation 

tokenized_textes %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col()  + 
  labs(title = "Les mots apparaissant plus de 5 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))

# Nombre total de lignes après tokenisation
nrow(tokenized_textes)
#Comme vous pouvez le voir sur le graphique ci-dessus, de nombreux mots présents n’apportent aucune réelle valeur à notre analyse. Des mots comme "de", "pour", "les", "le", "la" sont ce qu’on appelle des mots vides (stop words).

#Nous allons supprimer ces mots en utilisant la commande anti_join(stop_words).

# Charger les stop words en français
stop_words_fr <- tibble(word = stopwords("fr")) 


Stop_texte <- tokenized_textes %>% 
  anti_join(stop_words_fr, by = "word")

nrow(Stop_texte)


#Comme vous pouvez le voir sur le graphique ci-dessous, il reste moins de mots, mais ils sont beaucoup plus pertinents pour l’analyse.

Stop_texte %>%
  anti_join(stop_words_fr) %>% #finds where tweet words overlap with predefined stop words, and removes them
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col() + 
  labs(title = "Les mots apparaissant plus de 5 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))



library(ggwordcloud) # Une autre manière de visualiser

Stop_texte %>%
  anti_join(stop_words_fr) %>%
  count(word, sort = TRUE) %>%
  filter(n > 4) %>%
  ggplot(aes(label = word, size = n, color = n)) + 
  geom_text_wordcloud() + 
  scale_size_area(max_size = 15) 

# On peut également voir ci-dessous, la fréquence standard des termes (TF) pour tous les mots

Stop_texte %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  mutate(total=sum(count))%>%
  mutate(tf=count/total) %>%
  head()


# Racinisation

# Le nombre de mot avant le stemming
Stop_texte %>%
  count(word, sort = TRUE)%>%
  nrow()

# stemming
Stop_texte = Stop_texte %>%
  mutate(stem = wordStem(word))

# unique count of words after stemming
Stop_texte %>%
  count(stem, sort = TRUE) %>%
  nrow()


# Fréquence des mots avant le stemming
Stop_texte %>%
  count(word) %>%
  arrange(desc(n)) %>%
  head(10)

# Fréquence des mots après le stemming
Stop_texte %>%
  count(stem) %>%
  arrange(desc(n)) %>%
  head(10)

"
On remarque que le stemming ne semble pas respecter la logique de certains mots.
En effet, la racinisation supprime les lettres 's' à la fin des mots comme 'plus' et 'temps'.
Egalement le mot 'paix' est réduit à 'pai'.

"



# Ci-dessous, nous voyons l'intégralité du tableau TF-IDF. 
# Ce qui nous intéresse le plus, c'est la colonne tf_idf, car elle nous donne le classement pondéré ou l'importance des mots dans notre texte.

texte_tf_idf <- Stop_texte %>%
  count(word, id, sort = TRUE) %>%
  rename(count = n) %>%
  bind_tf_idf(word, id, count)
head(texte_tf_idf)


# Les simples décomptes de fréquences de mots peuvent être trompeurs et peu utiles
# pour bien comprendre vos données. Démontrons cela ci-dessous.

texte_tf_idf %>%
  select(word, id, tf_idf, count) %>%
  group_by(id) %>%
  slice_max(order_by = count, n = 6, with_ties=FALSE) %>% #takes top 5 words from each
  filter(id < 6) %>% #just look at 5 tweets
  ggplot(aes(label = word)) + 
  geom_text_wordcloud() + 
  facet_grid(rows = vars(id))


texte_tf_idf %>%
  select(word, id, tf_idf) %>%
  group_by(id) %>%
  slice_max(order_by = tf_idf,n = 6, with_ties=FALSE) %>% #takes top 5 words from each tweet
  filter(id < 6) %>% #just look at 5 tweets
  ggplot(aes(label = word)) + 
  geom_text_wordcloud() + 
  facet_grid(rows = vars(id))


## Relations entre les mots

"
Jusqu'à présent, nous avons seulement examiné les mots individuellement. Mais que faire si nous voulons connaître les relations entre les mots dans un texte ? 
Cela peut être accompli grâce aux n-grammes, où n est un nombre.
Auparavant, nous avions effectué une tokenisation mot par mot, mais nous pouvons aussi tokeniser par groupes de n mots. Créons maintenant des bigrams (groupes de deux mots) à partir de tous les tweets, puis comptons-les et trions-les.
"

textes_bigram <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2) 
head(textes_bigram)

"
Comme vous pouvez le voir dans le dataframe ci-dessus, certains bigrammes contiennent des mots vides (stop words) qui n’apportent pas beaucoup de valeur. 
Supprimons ces mots vides. Pour cela, nous allons d'abord séparer la colonne des bigrammes en deux colonnes distinctes nommées 'word1' et 'word2'. 
Ensuite, nous utiliserons deux fonctions de filtre pour supprimer les mots vides.
"
textes_bigram <- textes_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%#separates on whitespace
  filter(!word1 %in% stop_words_fr$word) %>%
  filter(!word2 %in% stop_words_fr$word)

head(textes_bigram)

"On peut maintenant compter les bigram et voir le résultat"
bigram_counts <- textes_bigram %>%
  count(word1, word2, sort = TRUE)
head(bigram_counts)

"
Comme précédemment, on peut aussi créer une mesure TF-IDF avec des n-grammes.
Faisons-le maintenant
"

data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2) %>%
  count(id, bigram) %>%
  bind_tf_idf(bigram, id, n) %>%
  group_by(id) %>%
  arrange(id, desc(tf_idf)) %>%
  head()

"
Comme on peut le voir ci-dessus, beaucoup de valeurs TF-IDF sont identiques.
Cela est en partie dû à la petite taille des textes.
Jetons maintenant un coup d'œil visuel aux relations entre 
les mots dans tous les textes, en utilisant un graphe en réseau.
"

library('igraph')
library('ggraph')
bi_graph <- bigram_counts %>%
  filter(n > 1) %>% 
  graph_from_data_frame()

ggraph(bi_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

"
Comme on peut le voir ci-dessus, de nombreux noms et d’autres 
informations ont été extraits des données
"

texte_trigram <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% #separates on whitespace
  filter(!word1 %in% stop_words_fr$word) %>%
  filter(!word2 %in% stop_words_fr$word) %>%
  filter(!word3 %in% stop_words_fr$word)

head(texte_trigram)

"On peut aussi compter les trigram et voir le résultat"

trigram_counts <- tweets_trigram %>%
  count(word1, word2, word3, sort = TRUE)
head(trigram_counts)

"Comme précédemment, on peut aussi créer une mesure TF-IDF avec des trigrammes. Faisons-le maintenant."

data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
  count(id, trigram) %>%
  bind_tf_idf(trigram, id, n) %>%
  group_by(id) %>%
  arrange(id, desc(tf_idf)) %>%
  head()


"
Comme on peut le voir ci-dessus, beaucoup de valeurs TF-IDF sont identiques.
Cela est en partie dû à la petite taille des textes comme remarqué dans le cas bigram.
Jetons maintenant un coup d'œil visuel aux relations entre les mots dans
l’ensemble des textes, en utilisant un graphe en réseau.

"

tri_graph <- trigram_counts %>%
  filter(n > 0) %>% # Ici, on garde TOUS les trigrammes présents au moins une fois
  graph_from_data_frame()

ggraph(tri_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

"

Normalement, on mettrait n > 1 ou n > 2 pour filtrer les trigrammes peu fréquents.
MAIS : dans notre cas, les tweets sont très courts, donc les trigrammes se répètent très peu.
Résultat : quasiment aucun trigramme n’apparaît plus d’une fois.
Du coup, si on filtre avec n > 1 ou n > 2 → le graphe devient vide (aucun trigramme à afficher).

En mettant n > 0, on garde tous les trigrammes possibles, même ceux présents une seule fois.
Cela permet d’obtenir un graphe, même si les connexions sont faibles (juste 1 apparition).

"

## Topic Modeling

"
Il est courant d’avoir une collection de documents, comme des articles de presse ou des publications sur 
les réseaux sociaux, que l’on souhaite diviser en thèmes. Autrement dit, on veut savoir quel est le sujet principal 
dans chaque document. Cela peut se faire grâce à une technique appelée modélisation thématique (topic modeling).
Ici, nous allons explorer la modélisation thématique à travers la méthode LDA (Latent Dirichlet Allocation).

LDA repose sur deux grands principes :
Chaque document est un mélange de plusieurs sujets

Chaque sujet est un mélange de mots

Un exemple classique serait de supposer qu’il existe deux grands sujets dans 
les actualités : la politique et le divertissement.
Le sujet politique contiendra des mots comme élu, gouvernement,

Tandis que le sujet divertissement contiendra des mots comme film, acteur.
Mais certains mots peuvent apparaître dans les deux, comme prix ou budget.

LDA va identifier :
les mélanges de mots qui composent chaque sujet, et
les mélanges de sujets qui composent chaque document.
Voyons cela à travers un exemple :
On commence par créer notre modèle LDA.
La fonction LDA() nécessite en entrée une matrice document-terme (DocumentTermMatrix), que l’on peut 
créer à partir de notre TF-IDF que nous avons généré précédemment.

"



"
Une fois que nous avons créé notre modèle de sujets LDA, nous pouvons 
utiliser la fonction tidy() pour le convertir en un tibble facile à lire et à manipuler.

La colonne beta générée correspond à la probabilité mot-par-sujet, c’est-à-dire
la probabilité qu’un mot soit généré à partir d’un sujet donné.

"







"

Parfait ! Nous avons maintenant un tibble simple et facile à utiliser.
Nous avons la probabilité que chaque mot apparaisse dans chaque sujet.

Maintenant, nous allons travailler sur la visualisation des mots de chaque sujet.
Il est particulièrement utile ici de trouver les 10 mots les plus représentatifs de chaque
sujet, afin de mieux comprendre le contenu de chaque thème.

Pour cela, il faut d’abord extraire les 10 mots les plus probables pour chaque sujet.
Cela peut se faire en utilisant quelques verbes dplyr, comme dans l’exemple ci-dessous.

"



"
Maintenant que nous avons les 10 termes les plus représentatifs par 
sujet, nous pouvons les visualiser afin de mieux comprendre de quoi parle chaque sujet.
"



# 1. Extraire les ID avant prétraitement (tweetsDF)
Id_initial_texte <- unique(Texte_JI$id)
length(Id_initial_texte)

# 2. Extraire les ID après prétraitement (text_df)
Id_final_texte <- unique(Stop_texte$id)
length(Id_final_texte)


#. Identifier les tweets manquants (présents avant, absents après)
Id_texte_NA <- setdiff(Id_initial_texte, Id_final_texte)

"
Dans notre base de données, le champ contenant les suggestions n’est pas obligatoire, ce qui signifie 
que plusieurs enregistrements présentent des valeurs manquantes (NA). 
Lors de l’application initiale du modèle LDA sur l’ensemble de la base, nous avons constaté 
que certains textes vides étaient malgré tout classés dans une catégorie, simplement 
parce qu’ils étaient présents dans les données en entrée. En d’autres termes, le modèle attribuait 
un sujet à un champ vide, ce qui n’a pas de sens et fausse l’interprétation : dans la nouvelle variable 
contenant les catégories issues du LDA, on retrouvait ainsi des lignes avec un texte vide associé à une thématique, comme si le modèle 
avait 'catégorisé du vide'.

Pour éviter ce biais, nous avons adopté une nouvelle approche plus rigoureuse. 
Nous avons d’abord isolé les textes non vides, c’est-à-dire les enregistrements contenant effectivement une suggestion. 
Le modèle LDA a donc été appliqué uniquement sur cette sous-base, ce qui garantit que chaque catégorisation repose sur un contenu textuel réel.



En parallèle, nous avons soigneusement conservé les identifiants (IDs) des textes vides, afin de pouvoir
les réintégrer dans la base complète après classification. Cela permet de reconstituer une base cohérente, où :

les textes contenant une suggestion sont associés à une catégorie issue du LDA,

et les textes vides conservent leur place, avec éventuellement une étiquette neutre comme 'Non renseigné' ou NA dans la variable de catégorie.

Cette méthode permet ainsi de respecter la structure initiale de la base, d’éviter 
des classifications erronées sur des données absentes, et de garantir une analyse fiable et interprétable.

"

"
Récupérons d'abord les ID des textes vides pour les réutiliser.

"

# 1. Extraire les ID avant prétraitement (tweetsDF)
Id_initial_texte <- unique(Texte_JI$id)
length(Id_initial_texte)

# 2. Extraire les ID après prétraitement (text_df)
Id_final_texte <- unique(Stop_texte$id)
length(Id_final_texte)


#. Identifier les Id des textes manquants
Id_texte_NA <- setdiff(Id_initial_texte, Id_final_texte)

# création d'une matrice document-thème pour LDA
df_dtm <- Stop_texte %>%
  count(id, word) %>%              
  cast_dtm(id, word, n)  



"
Dans le cadre de la modélisation thématique avec LDA (Latent Dirichlet Allocation), un des éléments clés du paramétrage est le choix du nombre de thèmes (K).
Ce paramètre n’est pas déterminé automatiquement par le modèle ; il doit être choisi par l’utilisateur, en fonction des données et des objectifs de l’analyse.
Or, le nombre de thèmes a un impact direct sur la qualité et la lisibilité du modèle :

Un K trop petit risque de regrouper des thématiques très différentes dans un même sujet, rendant le résultat peu précis.

Un K trop grand peut sur-segmenter les données, en produisant des thèmes trop spécifiques ou redondants, souvent difficiles à interpréter.


C’est pourquoi il est important de trouver un équilibre, c’est-à-dire un K optimal qui capte suffisamment de variété sans trop complexifier le modèle.

Afin de déterminer lenombre K optimal de thèmes, on utilisons la perplexité du modèle pour plusieurs valeurs de K
La perplexité est une mesure standard issue de la modélisation probabiliste, souvent utilisée pour évaluer les modèles de langage.
Dans le contexte de LDA, elle mesure dans quelle mesure le modèle 'explique' les données textuelles

"

# Étape 2 : Appliquer la LDA. Choix de k______________________________________


# Tester plusieurs valeurs de K (par exemple, de 2 à 10)
k_values <- 2:10
perplexities <- sapply(k_values, function(k) {
  lda_model <- LDA(df_dtm, k = k, control = list(seed = 1234))
  perplexity(lda_model)
})

# Afficher le graphique
ggplot(data.frame(K = k_values, Perplexity = perplexities), aes(x = K, y = Perplexity)) +
  geom_point() + geom_line() +
  labs(title = "Choix du K optimal avec la perplexité",
       x = "Nombre de thèmes (K)", y = "Perplexité")


"
Le graphique montre une forte diminution de la perplexité entre K = 2 et K = 7, ce qui indique 
que chaque thème ajouté dans cette plage apporte une réelle amélioration du modèle. Ensuite, à partir de K ≈ 8, la courbe 
commence à s’aplatir : les gains supplémentaires deviennent de plus en plus faibles.

Ce comportement suggère qu’à partir de K = 8, ajouter davantage de thèmes n’améliore 
plus significativement la qualité du modèle, tout en augmentant sa complexité. 
On peut donc considérer K = 8 comme un bon compromis, car il permet de 
capter une diversité raisonnable de thématiques sans trop fragmenter les données.

Cela justifie donc le choix de 8 thèmes comme valeur optimale dans notre modélisation LDA.

"


"
En théorie, la perplexité est censée diminuer à mesure que le nombre de thèmes (K) augmente. 
En effet, un modèle avec plus de thèmes dispose de plus de 'flexibilité' pour représenter les textes de manière fine. 
Cela se traduit généralement par une meilleure capacité à prédire les mots observés dans les documents — donc une perplexité plus faible.

Cependant, ce comportement n’est pas garanti dans tous les cas. 
Il peut arriver que la perplexité stagne voire augmente à partir d’un certain K, ou ne suive pas une baisse régulière. 
Ce phénomène peut être lié à plusieurs facteurs, notamment à la nature des textes analysés.

Un cas courant :

Les mots utilisés dans les documents peuvent être très variés même s’ils expriment des idées similaires. Par exemple, des mots comme 
gouvernement, État, autorités, institution peuvent tous renvoyer à la même notion 
politique, mais être traités comme des termes distincts par le modèle. Cela peut fragmenter artificiellement 
les thèmes, ou faire croire à une diversité de contenus plus grande qu’en réalité.

Dans ces situations, la perplexité peut ne plus refléter fidèlement la 'cohérence sémantique' des thèmes.
Elle devient donc une mesure limitée, surtout si les textes sont courts, informels ou s’ils contiennent beaucoup de synonymes ou paraphrases.

"

#__Voici un exemple appliquer aux données

# But : Réduire la complexité lexicale avant d’appliquer LDA.


text_df2 <- text_df1 %>%
  mutate(text_semantic = word) %>%  # dupliquer la colonne lemmatisée
  mutate(
    text_semantic = str_replace_all(text_semantic, "\\b(manger|plat|mets|piment|restaurant)\\b", "alimentation"),
    text_semantic = str_replace_all(text_semantic, "\\b(sketch|micro|temps|chanter|court|courtmétrage|sketcheval|scène|culture|culturel|lapprentissage|salle|présentation|apprendre|discours|métrage|fun|espace|prestataire|marrant|comique|communauté|aménager)\\b", "animation"),
    text_semantic = str_replace_all(text_semantic, "\\b(audible|sonorisation|technique)\\b", "technique"),
    text_semantic = str_replace_all(text_semantic, "\\b(communication|organisation|créativité|organiser)\\b", "Organisation")
  )


# Modélisation avec la valeur K=8

lda_model <- LDA(df_dtm, k = 8, control = list(seed = 1234))


# Termes par thème
terms_by_topic <- tidy(lda_model, matrix = "beta")
terms_by_topic

"
La colonne beta représente la probabilité
qu’un mot donné appartienne à un thème particulier. 
En d'autres termes, plus la valeur de beta est élevée 
pour un mot dans un thème, plus ce mot est représentatif de ce thème
"


# Afficher les 10 termes les plus probables pour chaque thème
top_terms <- terms_by_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%  # Prend exactement 10 termes par thème
  ungroup() %>%
  arrange(topic, -beta)  # Trie par thème et par probabilité décroissante

# Vérification du nombre de termes sélectionnés par thème
top_terms %>%
  count(topic) 

# Visualisation des termes les plus probables par thème
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Termes les plus probables par thème",
       x = "Probabilité (beta)", y = "Terme")

# Étape 4 : Classification des textes par thème
textes_gamma <- tidy(lda_model, matrix = "gamma")

"
Ici pour chaque texte, on peut voir la probabilité qu'il a d'appartenir à chacun des thèmes
"

# Afficher les textes avec leur thème dominant
textes_classified <- textes_gamma %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

# Nombre de texte dans chaque thème
textes_classified %>%
  count(topic)

# Visualisation des nombres de texte dans chaque thème
textes_classified %>%
  ggplot(aes(factor(topic), fill = factor(topic))) +
  geom_bar() +
  labs(title = "Distribution des thèmes dominants",
       x = "Thème", y = "Nombre de textes")


# Labelliser les thèmes

# Ajouter des libellés aux thèmes en fonction des mots dominants

textes_gamma <- textes_gamma %>%
  mutate(topic_label = case_when(
    topic == 1 ~ "Diversité culturelle et activités communes",
    topic == 2 ~ "Performances artistiques et intervention scénique",
    topic == 3 ~ "Présentation des cultures par les communautés",
    topic == 4 ~ "Aménagement de l'espace et la gestion du temps",
    topic == 5 ~ "Organisation",
    topic == 6 ~ "Suggestions d'amélioration",
    topic == 7 ~ "Organisation générale et impression globale",
    topic == 8 ~ "animation",
    TRUE ~ "Autre"
  ))



# Application du modèle BERTopic de python


"
BERTopic est un outil puissant de topic modeling (modélisation de sujets) 
qui permet d’extraire automatiquement des thèmes principaux à partir de 
textes non structurés. Il se distingue des approches classiques comme 
LDA par sa capacité à capturer des relations sémantiques en se basant sur
le texte et non des motstokenisés.

"


#  Installation de miniconda et chargement du package reticulate 
"
Pour utiliser les bibliothèques Python dans R (comme bertopic, sentence-transformers, etc.) 
qui sont nécessaire à notre analyse, on utilise le package reticulate, qui agit 
comme un pont entre R et Python.
Afin d’assurer que tout fonctionne dans un environnement propre et contrôlé, nous allons 
installer Miniconda, une version légère de Conda, qui sert à gérer les environnements Python.

"

library(reticulate)

# Voir l'environnement actif de reticulate

py_config()


# Installation des packages nécessaires dans l’environnement actif de reticulate
reticulate::py_install(
  packages = c("sentence-transformers", "hdbscan", "umap-learn", "bertopic"),
  pip = TRUE
)


# Importation des modules Python

"
Chaque module est lié à une fonctionnalité clé :
# - sentence_transformers : gestion des modèles d'embedding de texte
# - hdbscan : algorithme de clustering utilisé par BERTopic
# - bertopic : la librairie principale pour la modélisation de sujets
# - umap : utilisé pour projeter les embeddings dans un espace de plus faible dimension
"


sentence_transformers <- import("sentence_transformers")
hdbscan <- import("hdbscan")
bertopic <- import("bertopic")
umap <- import("umap")  # important pour fixer le random_state

# 🔤 Modèle d'embedding (changeable par d'autres plus bas)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-MiniLM-L6-v2")

"
Ici on utilise 'paraphrase-MiniLM-L6-v2', un modèle rapide et efficace
"

# Les variétés existents du modèle (plus puissants) :

# 🧮 Comparaison de modèles d'embedding pour BERTopic
embedding_models <- data.frame(
  Modele = c(
    "paraphrase-distilbert-base-nli-stsb",
    "bert-base-nli-mean-tokens",
    "all-mpnet-base-v2"
  ),
  Taille = c(768, 768, 768),
  Precision_Semantique = c(
    "Bonne précision sémantique",
    "Très bonne précision sémantique",
    "Excellente précision sémantique"
  )
)

# 📊 Affichage du tableau
knitr::kable(embedding_models, caption = "Tableau comparatif de modèles d'embedding utilisables avec BERTopic")


# 🔧 Clustering HDBSCAN
hdbscan_model <- hdbscan$HDBSCAN(
  min_cluster_size = reticulate::r_to_py(3L),
  min_samples = reticulate::r_to_py(1L)
)

# 🎯 Réduction de dimension via UMAP avec seed fixée pour reproductibilité
umap_model <- umap$UMAP(
  n_neighbors = 15L,
  n_components = 5L,
  min_dist = 0.0,
  metric = "cosine",
  random_state = 42L  # ✅ Seed fixée ici
)

# 📚 Création du modèle BERTopic
topic_model <- bertopic$BERTopic(
  language = "french",
  embedding_model = embedding_model,
  hdbscan_model = hdbscan_model,
  umap_model = umap_model
)

# 📦 Préparation des données

docs <- Texte_JI_filtered$Texte
ids <- Texte_JI_filtered$id  # on garde l'id associé à chaque texte

result <- topic_model$fit_transform(docs)


# 🎯 Extraction des résultats
topics <- result[[1]]
probs <- result[[2]]


# ➕ Ajout des résultats au data.frame d’origine
text_filtered$topic <- topics
text_filtered$topic_proba <- probs


# 🔁 Reconstruction du data.frame avec id + texte + classe
base_categorisee <- data.frame(
  id = ids,
  texte = docs,
  classe = topics,
  proba = probs
)


# 🔍 Affichage des infos sur les thèmes trouvés
topic_info <- topic_model$get_topic_info()
head(topic_info)

topic_info$label <- c(
  "Célébration et partage des cultures nationales",               # Topic 0
  "Aspect technique et organisation",                             # Topic 1
  "Mieux aménager l'espace",                                      # Topic 2
  "Sketchs et prestations",                                       # Topic 3
  "Gestion du timing lors des interventions",                     # Topic 4            
  "Touche créative et courmétrages"                                 # Topic 5
)


base_categorisee <- merge(
  base_categorisee, 
  topic_info[, c("Topic", "label")], 
  by.x = "classe", 
  by.y = "Topic", 
  all.x = TRUE
)


base_categorisee <- subset(base_categorisee, select = -c(classe, proba))


doc_vides <- data.frame(
  id = Id_texte_NA
)


# 2. Identifier les noms des autres colonnes (sauf "document")
autres_colonnes <- setdiff(names(base_categorisee), "id")

# 3. Ajouter des NA pour les autres colonnes
doc_vides[autres_colonnes] <- NA

# 4. Fusionner avec la base existante
textes_by_topic_complet <- rbind(base_categorisee, doc_vides)

View(textes_by_topic_complet)

# 5. Optionnel: trier par document si nécessaire
textes_by_topic_complet <- textes_by_topic_complet[order(textes_by_topic_complet$id), ]

# Joindre les thèmes dominants avec les tweets originaux

Texte_JI$Texte <- NULL

textes_classified <- Texte_JI %>%
  inner_join(textes_by_topic_complet, by = c("id" = "id"))

# Afficher la nouvella base

View(textes_classified)
getwd()
