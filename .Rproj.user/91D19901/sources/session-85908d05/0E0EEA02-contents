---
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
    toc: false
    number_sections: false
header-includes:
  - \usepackage{hyperref}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{graphicx}
  - \usepackage{fontspec}
  - \setmainfont{Cambria}
  - \setsansfont{Franklin Gothic Demi Cond}
  - \setmonofont{Courier New}
  - \usepackage[margin=1in]{geometry}
  - \usepackage{titlesec}
  - \titleformat{\section}{\Huge\bfseries\color{black}}{\thesection}{1em}{}
  - \titleformat{\subsection}{\huge\bfseries\color{black}}{\thesubsection}{1em}{}
  - \titleformat{\subsubsection}{\LARGE\bfseries\color{black}}{\thesubsubsection}{1em}{}
  - \usepackage{tocloft}
  - \renewcommand{\cftsecfont}{\small}   
  - \renewcommand{\cftsubsecfont}{\footnotesize} 
  - \renewcommand{\cftsecpagefont}{\small}   
  - \renewcommand{\cftsubsecpagefont}{\footnotesize}
  - \renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{=tex}
\begin{titlepage}
    \begin{center}
        \textbf{\LARGE R√âPUBLIQUE DU S√âN√âGAL}\\[0.1cm]
        \includegraphics[width=3cm]{images/Logo1.jpg} \\[0.1cm]  % Ins√®re le chemin de ton logo
        \textbf{\large Un Peuple - Un But - Une Foi}\\[0.2cm]
        
        \textbf{\LARGE Minist√®re de l'√âconomie, du Plan et de la Coop√©ration}\\[0.1cm]
        \includegraphics[width=4cm]{images/Logo2.png} \\[0.1cm] 
        
        \textbf{\large Agence Nationale de la Statistique et de la D√©mographie (ANSD)}\\[0.2cm]
        
        \includegraphics[width=4cm]{images/Logo3.png} \\[0.1cm]  
        
        \textbf{\large √âcole Nationale de la Statistique et de l'Analyse √âconomique Pierre Ndiaye (ENSAE)}\\[0.4cm]
        \includegraphics[width=3cm]{images/Logo4.png} \\[0.1cm]
        
        \textbf{\LARGE PROJET STATISTIQUES SOUS R }\\[0.3cm]
        \textbf{\Huge \color{blue} \textsf{TP10 : Traitement des questions ouvertes avec R : Text mining}}\\[0.2cm]
        \rule{\linewidth}{0.2mm} \\[0.5cm]
        
        \begin{minipage}{0.5\textwidth}
    \begin{flushleft} \large
        \emph{\textsf{R√©dig√© par :}}\\
        \textbf{Mame Balla BOUSSO}\\
        \textbf{Paul BALAFAI}\\
        \textit{El√®ves ing√©nieurs statisticiens √©conomistes}
    \end{flushleft}
\end{minipage}
        \hfill
        \begin{minipage}{0.4\textwidth}
            \begin{flushright} \large
                \emph{\textsf{Sous la supervision de :}} \\
                \textbf{M. Aboubacar HEMA}\\
                \textit{ANALYSTE DE RECHERCHE CHEZ IFPRI }
            \end{flushright}
        \end{minipage}

        \vfill

        {\large \textsf{Ann√©e scolaire : 2024/2025}}\\[0.5cm]
        
    \end{center}
\end{titlepage}
```

\newpage

# Sommaire

- [Traitement des questions ouvertes avec R](#analyse-textuelle-reponses-enquete)
  - [1. Importation et Nettoyage des Donn√©es](#importation-nettoyage-donnees)
  - [2. Exploration et Pr√©traitement Textuel](#exploration-pretraitement-textuel)
  - [3. Analyse Th√©matique (LDA)](#analyse-thematique-lda)
  - [4. Approche Alternative avec BERTopic](#approche-alternative-bertopic)
  - [5. Cat√©gorisation](#visualisation)
  - [CONCLUSION](#conclusion)
- [R√©f√©rences](#references-webographiques)

\newpage


\section{INTRODUCTION}\label{sec:importation}

Le traitement automatique du langage naturel (TALN) regroupe un ensemble de techniques permettant d‚Äôanalyser, de comprendre et de transformer des textes en donn√©es exploitables. Il existe plusieurs mani√®res d‚Äôaborder le traitement de texte, selon la nature des donn√©es et les objectifs vis√©s.

La forme la plus courante est l‚Äôanalyse **supervis√©e**, o√π chaque texte est associ√© √† un label pr√©d√©fini. Ces labels peuvent par exemple repr√©senter des cat√©gories binaires comme 0 ou 1, ou encore des sentiments comme positif, n√©gatif ou neutre. Dans ce contexte, on entra√Æne un mod√®le √† apprendre ces correspondances pour ensuite classer de nouveaux textes.

Cependant, dans de nombreux cas, notamment dans les **questions ouvertes d‚Äôenqu√™tes**, il n'existe aucune annotation pr√©alable permettant de guider l‚Äôapprentissage. Il devient alors n√©cessaire de structurer les donn√©es ex nihilo, c‚Äôest-√†-dire sans rep√®re pr√©alable, en regroupant les textes selon leur **similarit√© s√©mantique**. C‚Äôest le domaine de l‚Äôanalyse **non supervis√©e**.

Dans ce cas, une solution consiste √† effectuer une analyse th√©matique, en divisant le corpus en un **nombreùêæ**
de th√®mes ou sujets, choisi avec soin. Cette d√©marche vise √† extraire les grandes lignes du contenu textuel, en r√©v√©lant les motifs r√©currents pr√©sents dans les r√©ponses. Cela permet d‚Äôenrichir l‚Äôanalyse qualitative des enqu√™tes, m√™me en l‚Äôabsence de cat√©gorisation pr√©alable.

Dans cette √©tude, nous nous concentrerons sp√©cifiquement sur cette approche **non supervis√©e**. Nous mettrons en ≈ìuvre des techniques disponibles dans le langage R, notamment gr√¢ce √† des packages comme **topicmodels (pour le mod√®le LDA)**, **tidytext**, et **BERTopic** via le package **reticulate**, pour recourir √† des mod√®les s√©mantiques plus avanc√©s.

\newpage



\section{1. Importation et Nettoyage des Donn√©es}\label{sec:importation}


### Chargement des packages

```{r, warning=FALSE, message=FALSE}

library(readxl)       # Pour lire les fichiers Excel
library(topicmodels)  # Pour la mod√©lisation th√©matique
library(ggplot2)      # Pour les visualisations
library(dplyr)        # Pour la manipulation de donn√©es
library(tidytext)     # Pour le traitement de texte
library(tidyr)        # Pour la gestion des donn√©es
library(wordcloud)    # Pour les nuages de mots
library(tidyverse)    # Collection de packages pour la science des donn√©es
library(tm)           # Pour le text mining
library(SnowballC)    # Pour le stemming
library(stringr)      # Pour la manipulation de strings

```


##  Importation des donn√©es

```{r, warning=FALSE, message=FALSE}

Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration_ <- read_excel("Data/Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration).xlsx")
Texte_JI <- Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration_
head(Enqu√™te_dopinion_relative_√†_la_journ√©e_dint√©gration_)

```


#### V√©rification des colonnes

```{r}
colnames(Texte_JI)
```



### Identification des textes vides

```{r, warning=FALSE, message=FALSE}

text_id_empty <- Texte_JI %>%
  group_by(id) %>%
  summarise(nb_mots = sum(!is.na(Texte))) %>%
  filter(nb_mots == 0) %>%
  pull(id)

Texte_JI_filtered <- Texte_JI %>% 
  filter(!(id %in% text_id_empty))

head(Texte_JI_filtered)
nrow(Texte_JI_filtered)


```

On remarque que le nombre de ligne a diminu√© passant de 128 √† 45. Seulement 45 lignes contiennent des textes.


## Nettoyage des textes

On cr√©e une fonction pour traiter les textes afin de faciliter leur analyse

```{r, warning=FALSE, message=FALSE}
clean_text <- function(text) {
  text <- tolower(text)           # Conversion en minuscules
  text <- removePunctuation(text) # Suppression de la ponctuation
  text <- removeNumbers(text)     # Suppression des chiffres
  text <- stripWhitespace(text)   # Suppression des espaces superflus
  return(text)
}

data <- Texte_JI_filtered %>%
  mutate(Texte_corrige = sapply(Texte, clean_text))

```


\section{2. Exploration et Pr√©traitement des textes}\label{sec:importation}

Dans le processus de pr√©traitement des donn√©es, on va tokeniser la base de donn√©es pour analyser non pas les textes, mais les mots directement.

```{r, warning=FALSE, message=FALSE}
tokenized_textes <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(input = 'Texte_corrige', output = 'word')


# Visualisation Tokenisation 

tokenized_textes %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col()  + 
  labs(title = "Les mots apparaissant plus de 5 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))

# Nombre total de lignes apr√®s tokenisation
nrow(tokenized_textes)

```


Comme nous pouvons le voir sur le graphique ci-dessus, de nombreux mots pr√©sents n‚Äôapportent aucune r√©elle valeur √† notre analyse. Des mots comme **de**, **pour**, **les**, **le**, **la** sont ce qu‚Äôon appelle des mots vides (stop words).

Nous allons supprimer ces mots en utilisant la commande *anti_join(stop_words)*.

### Charger les stop words en fran√ßais

```{r, warning=FALSE, message=FALSE}

stop_words_fr <- tibble(word = stopwords("fr")) 
head(stop_words_fr)

```


```{r, warning=FALSE, message=FALSE}
Stop_texte <- tokenized_textes %>% 
  anti_join(stop_words_fr, by = "word")

nrow(Stop_texte)
```
On voit bien que le nombre de mots diminue suite √† la supression des 
stop word


Comme nous pouvons le voir sur le graphique ci-dessous, il reste moins de mots, mais ils sont beaucoup plus pertinents pour l‚Äôanalyse.


```{r, warning=FALSE, message=FALSE}
Stop_texte %>%
  anti_join(stop_words_fr) %>% # trouve l√† o√π les textes rencontrent des stop words, et les supprime
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
  geom_col() + 
  labs(title = "Les mots apparaissant plus de 5 fois") + 
  scale_x_continuous(breaks = seq(0, 50, 5))


```



```{r, warning=FALSE, message=FALSE}
library(ggwordcloud) # Une autre mani√®re de visualiser

Stop_texte %>%
  anti_join(stop_words_fr) %>%
  count(word, sort = TRUE) %>%
  filter(n > 4) %>%
  ggplot(aes(label = word, size = n, color = n)) + 
  geom_text_wordcloud() + 
  scale_size_area(max_size = 15) 

```

On peut √©galement voir ci-dessous, la fr√©quence standard des termes (TF) pour tous les mots


```{r, warning=FALSE, message=FALSE}

Stop_texte %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  mutate(total=sum(count))%>%
  mutate(tf=count/total) %>%
  head()
```


L'application des stop word diminue le nombre de mots. Ceci le montre

```{r, warning=FALSE, message=FALSE}

Stop_texte %>%
  count(word, sort = TRUE)%>%
  nrow()

```


### Racinisation
En racinisant, les mots *cultures* et *culture* par exemple se r√©duisent en *culture*. Voil√† pourquoi le nombre total de mots diminue comme le r√©sultat de cette commande l'illustre :


```{r, warning=FALSE, message=FALSE}
# stemming
Stop_texte = Stop_texte %>%
  mutate(stem = wordStem(word))

# unique count of words after stemming
Stop_texte %>%
  count(stem, sort = TRUE) %>%
  nrow()

```

Fr√©quence des mots avant le stemming

```{r, warning=FALSE, message=FALSE}
# Fr√©quence des mots avant le stemming
Stop_texte %>%
  count(word) %>%
  arrange(desc(n)) %>%
  head(10)

```

Fr√©quence des mots apr√®s le stmming

```{r, warning=FALSE, message=FALSE}
# Fr√©quence des mots apr√®s le stemming
Stop_texte %>%
  count(stem) %>%
  arrange(desc(n)) %>%
  head(10)

```


On remarque que le stemming ne semble pas respecter la logique pour certains mots.
En effet, la racinisation supprime les lettres *s* √† la fin des mots comme *plus* et *temps*.
Egalement le mot *paix* est r√©duit √† *pai*. Cela constitut une limite majeure quant √† la racinisation en langue fran√ßaise. C'est pourquoi dans ce qui suit, nous ferons fi de cette √©tape du pr√©traitement en utilisanat d√©sormais seulement mes textes issus de l'application des stop word.


### les analyse TF-IDF

Ci-dessous, nous voyons l'int√©gralit√© du tableau TF-IDF. 
Ce qui nous int√©resse le plus, c'est la colonne tf_idf, car elle nous donne le classement pond√©r√© ou l'importance des mots dans notre texte.

```{r, warning=FALSE, message=FALSE}

texte_tf_idf <- Stop_texte %>%
  count(word, id, sort = TRUE) %>%
  rename(count = n) %>%
  bind_tf_idf(word, id, count)
head(texte_tf_idf)


```


Les simples d√©comptes de fr√©quences de mots peuvent √™tre trompeurs et peu utiles
pour bien comprendre nos donn√©es. Il est en fait int√©ressant de voir les mots les plus 
fr√©quents dans chaque texte.

```{r, warning=FALSE, message=FALSE}

texte_tf_idf %>%
  select(word, id, tf_idf, count) %>%
  group_by(id) %>%
  slice_max(order_by = count, n = 6, with_ties=FALSE) %>% #takes top 5 words from each text
  filter(id < 6) %>% #just look at 5 textes
  ggplot(aes(label = word)) + 
  geom_text_wordcloud() + 
  facet_grid(rows = vars(id))

```


On s'est limit√© au cinq premiers textes. Mais les textes correspondant aux identifiants 1 et 3 sont des NA et donc ont √©t√© isol√©s.

Par ailleurs le r√©sultat qui suit montre aussi qu'il ne faut pas se limiter √† un simple d√©nombrement des textes mais √† leur fr√©quence.

```{r, warning=FALSE, message=FALSE}

texte_tf_idf %>%
  select(word, id, tf_idf) %>%
  group_by(id) %>%
  slice_max(order_by = tf_idf,n = 6, with_ties=FALSE) %>% #takes top 5 words from each text
  filter(id < 6) %>% #just look at 5 texts
  ggplot(aes(label = word)) + 
  geom_text_wordcloud() + 
  facet_grid(rows = vars(id))

```


### Relations entre les mots

Jusqu'√† pr√©sent, nous avons seulement examin√© les mots individuellement. Mais que faire si nous voulons conna√Ætre les relations entre les mots dans un texte ? 
Cela peut √™tre accompli gr√¢ce aux n-grammes, o√π n est un nombre.
Auparavant, nous avions effectu√© une tokenisation mot par mot, mais nous pouvons aussi tokeniser par groupes de n mots. Cr√©ons maintenant des bigrams (groupes de deux mots) √† partir de tous les textes, puis comptons-les et trions-les.

```{r, warning=FALSE, message=FALSE}

textes_bigram <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2) 
head(textes_bigram)


```


Comme vous pouvez le voir dans le dataframe ci-dessus, certains bigrammes contiennent des mots vides (stop words) qui n‚Äôapportent pas beaucoup de valeur. 
Supprimons ces mots vides. Pour cela, nous allons d'abord s√©parer la colonne des bigrammes en deux colonnes distinctes nomm√©es 'word1' et 'word2'. 
Ensuite, nous utiliserons deux fonctions de filtre pour supprimer les mots vides.


```{r, warning=FALSE, message=FALSE}
textes_bigram <- textes_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%#separates on whitespace
  filter(!word1 %in% stop_words_fr$word) %>%
  filter(!word2 %in% stop_words_fr$word)

head(textes_bigram)


```


On peut maintenant compter les bigram et voir le r√©sultat


```{r, warning=FALSE, message=FALSE}
bigram_counts <- textes_bigram %>%
  count(word1, word2, sort = TRUE)
head(bigram_counts)
```


Comme pr√©c√©demment, on peut aussi cr√©er une mesure TF-IDF avec des n-grammes.
Faisons-le maintenant.

```{r, warning=FALSE, message=FALSE}
data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2) %>%
  count(id, bigram) %>%
  bind_tf_idf(bigram, id, n) %>%
  group_by(id) %>%
  arrange(id, desc(tf_idf)) %>%
  head()

```

Comme on peut le voir ci-dessus, beaucoup de valeurs TF-IDF sont identiques.
Cela est en partie d√ª √† la petite taille des textes.
Jetons maintenant un coup d'≈ìil aux relations entre 
les mots dans tous les textes, en utilisant un graphe en r√©seau.


```{r, warning=FALSE, message=FALSE}
library('igraph')
library('ggraph')
bi_graph <- bigram_counts %>%
  filter(n > 1) %>% 
  graph_from_data_frame()

ggraph(bi_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```


Comme on peut le voir ci-dessus, de nombreux noms et d‚Äôautres 
informations ont √©t√© extraits des donn√©es.


```{r, warning=FALSE, message=FALSE}
texte_trigram <- data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% #separates on whitespace
  filter(!word1 %in% stop_words_fr$word) %>%
  filter(!word2 %in% stop_words_fr$word) %>%
  filter(!word3 %in% stop_words_fr$word)

head(texte_trigram)
```

On peut aussi compter les trigram et voir le r√©sultat

```{r, warning=FALSE, message=FALSE}
trigram_counts <- texte_trigram %>%
  count(word1, word2, word3, sort = TRUE)
head(trigram_counts)
```


Comme pr√©c√©demment, on peut aussi cr√©er une mesure TF-IDF avec des trigrammes. Faisons-le maintenant.

```{r, warning=FALSE, message=FALSE}
data %>%
  select(id, Texte_corrige) %>%
  unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
  count(id, trigram) %>%
  bind_tf_idf(trigram, id, n) %>%
  group_by(id) %>%
  arrange(id, desc(tf_idf)) %>%
  head()
```

Comme on peut le voir ci-dessus, beaucoup de valeurs TF-IDF sont identiques.
Cela est en partie d√ª √† la petite taille des textes comme remarqu√© dans le cas bigram.
Jetons maintenant un coup d'≈ìil aux relations entre les mots dans
l‚Äôensemble des textes, en utilisant un graphe en r√©seau.


```{r, warning=FALSE, message=FALSE}
tri_graph <- trigram_counts %>%
  filter(n > 0) %>% # Ici, on garde TOUS les trigrammes pr√©sents au moins une fois
  graph_from_data_frame()

ggraph(tri_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```


Normalement, on mettrait n > 1 ou n > 2 pour filtrer les trigrammes peu fr√©quents, mais dans notre cas, les textets sont tr√®s courts, donc les trigrammes se r√©p√®tent tr√®s peu.

*R√©sultat : quasiment aucun trigramme n‚Äôappara√Æt plus d‚Äôune fois. Du coup, si on filtre avec n > 1 ou n > 2 ‚Üí le graphe devient vide (aucun trigramme √† afficher).

En mettant n > 0, on garde tous les trigrammes possibles, m√™me ceux pr√©sents une seule fois.
Cela permet d‚Äôobtenir un graphe, m√™me si les connexions sont faibles (juste 1 apparition).



Dans notre base de donn√©es, le champ contenant les suggestions n‚Äôest pas obligatoire, ce qui signifie que plusieurs enregistrements pr√©sentent des valeurs manquantes (NA). 
Lors de l‚Äôapplication initiale du mod√®le LDA sur l‚Äôensemble de la base, nous avons constat√© 
que certains textes vides √©taient malgr√© tout class√©s dans une cat√©gorie, simplement 
parce qu‚Äôils √©taient pr√©sents dans les donn√©es en entr√©e. En d‚Äôautres termes, le mod√®le attribuait un sujet √† un champ vide, ce qui n‚Äôa pas de sens et fausse l‚Äôinterpr√©tation : dans la nouvelle variable 
contenant les cat√©gories issues du LDA, on retrouvait ainsi des lignes avec un texte vide associ√© √† une th√©matique, comme si le mod√®le avait 'cat√©goris√© du vide'.

Pour √©viter ce biais, nous avons adopt√© une nouvelle approche plus rigoureuse. 
Nous avons d‚Äôabord isol√© les textes non vides, c‚Äôest-√†-dire les enregistrements contenant effectivement une suggestion. 
Le mod√®le LDA a donc √©t√© appliqu√© uniquement sur cette sous-base, ce qui garantit que chaque cat√©gorisation repose sur un contenu textuel r√©el.


En parall√®le, nous avons soigneusement conserv√© les identifiants (IDs) des textes vides, afin de pouvoir les r√©int√©grer dans la base compl√®te apr√®s classification. Cela permet de reconstituer une base coh√©rente, o√π :

* -les textes contenant une suggestion sont associ√©s √† une cat√©gorie issue du LDA,

* -les textes vides conservent leur place, avec √©ventuellement une √©tiquette neutre comme 'Non renseign√©' ou NA dans la variable de cat√©gorie.

Cette m√©thode permet ainsi de respecter la structure initiale de la base, d‚Äô√©viter 
des classifications erron√©es sur des donn√©es absentes, et de garantir une analyse fiable et interpr√©table.

```{r, warning=FALSE, message=FALSE}
# 1. Extraire les ID avant pr√©traitement (Texte_JI)
Id_initial_texte <- unique(Texte_JI$id)
length(Id_initial_texte)

# 2. Extraire les ID apr√®s pr√©traitement (Stop_texte)
Id_final_texte <- unique(Stop_texte$id)
length(Id_final_texte)

#. Identifier les tweets manquants (pr√©sents avant, absents apr√®s)
Id_texte_NA <- setdiff(Id_initial_texte, Id_final_texte)
length(Id_texte_NA)
```


\section{3. Analyse Th√©matique (LDA)}\label{sec:importation}

Il est courant d‚Äôavoir une collection de documents, comme des articles de presse ou des publications sur 
les r√©seaux sociaux, que l‚Äôon souhaite diviser en th√®mes. Autrement dit, on veut savoir quel est le sujet principal 
dans chaque document. Cela peut se faire gr√¢ce √† une technique appel√©e mod√©lisation th√©matique (topic modeling).
Ici, nous allons explorer la mod√©lisation th√©matique √† travers la m√©thode LDA (Latent Dirichlet Allocation).

LDA repose sur deux grands principes :
Chaque document est un m√©lange de plusieurs sujets

Chaque sujet est un m√©lange de mots

Un exemple classique serait de supposer qu‚Äôil existe deux grands sujets dans 
les actualit√©s : la politique et le divertissement.
Le sujet politique contiendra des mots comme √©lu, gouvernement,

Tandis que le sujet divertissement contiendra des mots comme film, acteur.
Mais certains mots peuvent appara√Ætre dans les deux, comme prix ou budget.

LDA va identifier :
les m√©langes de mots qui composent chaque sujet, et
les m√©langes de sujets qui composent chaque document.
Voyons cela √† travers un exemple :
On commence par cr√©er notre mod√®le LDA.
La fonction LDA() n√©cessite en entr√©e une matrice document-terme (DocumentTermMatrix), que l‚Äôon peut 
cr√©er √† partir de notre base d√©j√† pr√©trait√©e que nous avons g√©n√©r√© pr√©c√©demment.


### Cr√©ation d'une matrice document-th√®me

```{r, warning=FALSE, message=FALSE}
# cr√©ation d'une matrice document-th√®me pour LDA
df_dtm <- Stop_texte %>%
  count(id, word) %>%              
  cast_dtm(id, word, n)  

```


## Choix du nombre k de th√®mes

Dans le cadre de la mod√©lisation th√©matique avec LDA (Latent Dirichlet Allocation), un des √©l√©ments cl√©s du param√©trage est le choix du nombre de th√®mes (K).
Ce param√®tre n‚Äôest pas d√©termin√© automatiquement par le mod√®le ; il doit √™tre choisi par l‚Äôutilisateur, en fonction des donn√©es et des objectifs de l‚Äôanalyse.
Or, le nombre de th√®mes a un impact direct sur la qualit√© et la lisibilit√© du mod√®le :

Un K trop petit risque de regrouper des th√©matiques tr√®s diff√©rentes dans un m√™me sujet, rendant le r√©sultat peu pr√©cis.

Un K trop grand peut sur-segmenter les donn√©es, en produisant des th√®mes trop sp√©cifiques ou redondants, souvent difficiles √† interpr√©ter.


C‚Äôest pourquoi il est important de trouver un √©quilibre, c‚Äôest-√†-dire un K optimal qui capte suffisamment de vari√©t√© sans trop complexifier le mod√®le.

Afin de d√©terminer lenombre K optimal de th√®mes, on utilisons la perplexit√© du mod√®le pour plusieurs valeurs de K
La perplexit√© est une mesure standard issue de la mod√©lisation probabiliste, souvent utilis√©e pour √©valuer les mod√®les de langage.
Dans le contexte de LDA, elle mesure dans quelle mesure le mod√®le 'explique' les donn√©es textuelles



```{r, warning=FALSE, message=FALSE}
k_values <- 2:10
perplexities <- sapply(k_values, function(k) {
  lda_model <- LDA(df_dtm, k = k, control = list(seed = 1234))
  perplexity(lda_model)
})

# Afficher le graphique
ggplot(data.frame(K = k_values, Perplexity = perplexities), aes(x = K, y = Perplexity)) +
  geom_point() + geom_line() +
  labs(title = "Choix du K optimal avec la perplexit√©",
       x = "Nombre de th√®mes (K)", y = "Perplexit√©")


```



Le graphique montre une forte diminution de la perplexit√© entre K = 2 et K = 7, ce qui indique 
que chaque th√®me ajout√© dans cette plage apporte une r√©elle am√©lioration du mod√®le. Ensuite, √† partir de K ‚âà 8, la courbe 
commence √† s‚Äôaplatir : les gains suppl√©mentaires deviennent de plus en plus faibles.

Ce comportement sugg√®re qu‚Äô√† partir de K = 8, ajouter davantage de th√®mes n‚Äôam√©liore 
plus significativement la qualit√© du mod√®le, tout en augmentant sa complexit√©. 
On peut donc consid√©rer K = 8 comme un bon compromis, car il permet de 
capter une diversit√© raisonnable de th√©matiques sans trop fragmenter les donn√©es.

Cela justifie donc le choix de 8 th√®mes comme valeur optimale dans notre mod√©lisation LDA.

#### G√©n√©ralement

En th√©orie, la perplexit√© est cens√©e diminuer √† mesure que le nombre de th√®mes (K) augmente. 
En effet, un mod√®le avec plus de th√®mes dispose de plus de 'flexibilit√©' pour repr√©senter les textes de mani√®re fine. 
Cela se traduit g√©n√©ralement par une meilleure capacit√© √† pr√©dire les mots observ√©s dans les documents ‚Äî donc une perplexit√© plus faible.

Cependant, ce comportement n‚Äôest pas garanti dans tous les cas. 
Il peut arriver que la perplexit√© stagne voire augmente √† partir d‚Äôun certain K, ou ne suive pas une baisse r√©guli√®re. 
Ce ph√©nom√®ne peut √™tre li√© √† plusieurs facteurs, notamment √† la nature des textes analys√©s.

Un cas courant :

Les mots utilis√©s dans les documents peuvent √™tre tr√®s vari√©s m√™me s‚Äôils expriment des id√©es similaires. Par exemple, des mots comme **gouvernement**, **√âtat**, **autorit√©s**, **institution** peuvent tous renvoyer √† la m√™me notion 
politique, mais √™tre trait√©s comme des termes distincts par le mod√®le. Cela peut fragmenter artificiellement 
les th√®mes, ou faire croire √† une diversit√© de contenus plus grande qu‚Äôen r√©alit√©.

Dans ces situations, la perplexit√© peut ne plus refl√©ter fid√®lement la **coh√©rence s√©mantique** des th√®mes.
Elle devient donc une mesure limit√©e, surtout si les textes sont courts, informels ou s‚Äôils contiennent beaucoup de synonymes ou paraphrases.

### Une solution souvent utilis√©e : Groupage par th√®me


```{r, warning=FALSE, message=FALSE}

text_df2 <- Stop_texte %>%
  mutate(text_semantic = word) %>%  # dupliquer la colonne lemmatis√©e
  mutate(
    text_semantic = str_replace_all(text_semantic, "\\b(manger|plat|mets|piment|restaurant)\\b", "alimentation"),
    text_semantic = str_replace_all(text_semantic, "\\b(sketch|micro|temps|chanter|court|courtm√©trage|sketcheval|sc√®ne|culture|culturel|lapprentissage|salle|pr√©sentation|apprendre|discours|m√©trage|fun|espace|prestataire|marrant|comique|communaut√©|am√©nager)\\b", "animation"),
    text_semantic = str_replace_all(text_semantic, "\\b(audible|sonorisation|technique)\\b", "technique"),
    text_semantic = str_replace_all(text_semantic, "\\b(communication|organisation|cr√©ativit√©|organiser)\\b", "Organisation")
  )

```

### Limites : pas trop flexible surtout en cas de grands volumes de donn√©es
Dans ce qui suit, nous continuerons directement avec les donn√©es d√©j√† pr√©trait√©es et visualis√©es sans appliquer un groupage suppl√©mentaire.

```{r, warning=FALSE, message=FALSE}
lda_model <- LDA(df_dtm, k = 8, control = list(seed = 1234))

# Termes par th√®me
terms_by_topic <- tidy(lda_model, matrix = "beta")
terms_by_topic

```


La colonne beta repr√©sente la probabilit√© qu‚Äôun mot donn√© appartienne √† un th√®me particulier. 
En d'autres termes, plus la valeur de beta est √©lev√©e 
pour un mot dans un th√®me, plus ce mot est repr√©sentatif de ce th√®me

```{r, warning=FALSE, message=FALSE}
top_terms <- terms_by_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%  # Prend exactement 10 termes par th√®me
  ungroup() %>%
  arrange(topic, -beta)  # Trie par th√®me et par probabilit√© d√©croissante

# V√©rification du nombre de termes s√©lectionn√©s par th√®me
top_terms %>%
  count(topic) 
```

1O termes ont √©t√© s√©lectionn√©s par th√®mes. On peut les visualiser √©galement

```{r, warning=FALSE, message=FALSE}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Termes les plus probables par th√®me",
       x = "Probabilit√© (beta)", y = "Terme")

```



```{r, warning=FALSE, message=FALSE}
# √âtape 4 : Classification des textes par th√®me
textes_gamma <- tidy(lda_model, matrix = "gamma")
# Afficher les textes avec leur th√®me dominant
textes_classified <- textes_gamma %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

```


Ici pour chaque texte, on peut voir la probabilit√© qu'il a d'appartenir √† chacun des th√®mes.


```{r, warning=FALSE, message=FALSE}
# Nombre de texte dans chaque th√®me
textes_classified %>%
  count(topic)

```
Pour chaque th√®me, on voit le nombre de textes

```{r, warning=FALSE, message=FALSE}
# Visualisation des nombres de texte dans chaque th√®me
textes_classified %>%
  ggplot(aes(factor(topic), fill = factor(topic))) +
  geom_bar() +
  labs(title = "Distribution des th√®mes dominants",
       x = "Th√®me", y = "Nombre de textes")
```


### Labellisation (tr√®s subjective)

```{r, warning=FALSE, message=FALSE}
# Labelliser les th√®mes

textes_gamma <- textes_gamma %>%
  mutate(topic_label = case_when(
    topic == 1 ~ "Diversit√© culturelle et activit√©s communes",
    topic == 2 ~ "Performances artistiques et intervention sc√©nique",
    topic == 3 ~ "Pr√©sentation des cultures par les communaut√©s",
    topic == 4 ~ "Am√©nagement de l'espace et la gestion du temps",
    topic == 5 ~ "Organisation",
    topic == 6 ~ "Suggestions d'am√©lioration",
    topic == 7 ~ "Organisation g√©n√©rale et impression globale",
    topic == 8 ~ "animation",
    TRUE ~ "Autre"
  ))
```


\section{4. Approche Alternative avec BERTopic}\label{sec:importation}


BERTopic est un outil puissant de topic modeling (mod√©lisation de sujets) 
qui permet d‚Äôextraire automatiquement des th√®mes principaux √† partir de 
textes non structur√©s. Il se distingue des approches classiques comme 
LDA par sa capacit√© √† capturer des relations s√©mantiques en se basant sur
le texte et non des motstokenis√©s.



##  Installation de miniconda et chargement du package reticulate 

Pour utiliser les biblioth√®ques Python dans R (comme bertopic, sentence-transformers, etc.) 
qui sont n√©cessaire √† notre analyse, on utilise le package reticulate, qui agit 
comme un pont entre R et Python.
Afin d‚Äôassurer que tout fonctionne dans un environnement propre et contr√¥l√©, nous allons 
installer Miniconda, une version l√©g√®re de Conda, qui sert √† g√©rer les environnements Python.

```{r, warning=FALSE, message=FALSE}
library(reticulate)

# Voir l'environnement actif de reticulate

py_config()

# Installation des packages n√©cessaires dans l‚Äôenvironnement actif de reticulate
reticulate::py_install(
  packages = c("sentence-transformers", "hdbscan", "umap-learn", "bertopic"),
  pip = TRUE
)

```

### Importation des modules Python


Chaque module que nous importons est li√© √† une fonctionnalit√© cl√© :

- sentence_transformers : gestion des mod√®les d'embedding de texte
- hdbscan : algorithme de clustering utilis√© par BERTopic
- bertopic : la librairie principale pour la mod√©lisation de sujets
- umap : utilis√© pour projeter les embeddings dans un espace de plus faible dimension


```{r, warning=FALSE, message=FALSE}
sentence_transformers <- import("sentence_transformers")
hdbscan <- import("hdbscan")
bertopic <- import("bertopic")
umap <- import("umap")  # important pour fixer le random_state
```

```{r, warning=FALSE, message=FALSE}
# üî§ Mod√®le d'embedding (changeable par d'autres plus bas)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-MiniLM-L6-v2")

```

Ici on utilise 'paraphrase-MiniLM-L6-v2', un mod√®le rapide et efficace. Mais
il existe d'autres vari√©t√©s plus puissante mais qui sont plus robuste.
Le tableau qui suit donne quelques d√©tails.

```{r, warning=FALSE, message=FALSE}

# üßÆ Comparaison de mod√®les d'embedding pour BERTopic
embedding_models <- data.frame(
  Modele = c(
    "paraphrase-distilbert-base-nli-stsb",
    "bert-base-nli-mean-tokens",
    "all-mpnet-base-v2"
  ),
  Taille = c(768, 768, 768),
  Precision_Semantique = c(
    "Bonne pr√©cision s√©mantique",
    "Tr√®s bonne pr√©cision s√©mantique",
    "Excellente pr√©cision s√©mantique"
  )
)
# üìä Affichage du tableau
knitr::kable(embedding_models, caption = "Tableau comparatif de mod√®les d'embedding utilisables avec BERTopic")

```



```{r, warning=FALSE, message=FALSE}
hdbscan_model <- hdbscan$HDBSCAN(
  min_cluster_size = reticulate::r_to_py(3L),
  min_samples = reticulate::r_to_py(1L)
)

# üéØ R√©duction de dimension via UMAP avec seed fix√©e pour reproductibilit√©
umap_model <- umap$UMAP(
  n_neighbors = 15L,
  n_components = 5L,
  min_dist = 0.0,
  metric = "cosine",
  random_state = 42L  # ‚úÖ Seed fix√©e ici
)

# üìö Cr√©ation du mod√®le BERTopic
topic_model <- bertopic$BERTopic(
  language = "french",
  embedding_model = embedding_model,
  hdbscan_model = hdbscan_model,
  umap_model = umap_model
)

# üì¶ Pr√©paration des donn√©es

docs <- Texte_JI_filtered$Texte
ids <- Texte_JI_filtered$id  # on garde l'id associ√© √† chaque texte

result <- topic_model$fit_transform(docs)


# üéØ Extraction des r√©sultats
topics <- result[[1]]
probs <- result[[2]]

topics
probs

```


```{r, warning=FALSE, message=FALSE}
# üîÅ Reconstruction du data.frame avec id + texte + classe
base_categorisee <- data.frame(
  id = ids,
  texte = docs,
  classe = topics,
  proba = probs
)

# üîç Affichage des infos sur les th√®mes trouv√©s
topic_info <- topic_model$get_topic_info()
head(topic_info)
```


\section{5. Cat√©gorisation}\label{sec:importation}

Dans cette section, nous tentons de cat√©goriser les textes en se basant sur les diff'rents th√®mes g√©n√©r√©s par le mod√®le.


```{r, warning=FALSE, message=FALSE}
topic_info$label <- c(
  "C√©l√©bration et partage des cultures nationales",               # Topic 0
  "Aspect technique et organisation",                             # Topic 1
  "Mieux am√©nager l'espace",                                      # Topic 2
  "Sketchs et prestations",                                       # Topic 3
  "Gestion du timing lors des interventions",                     # Topic 4       
  "Touche cr√©ative et courm√©trages"                                  # Topic 5
)


base_categorisee <- merge(
  base_categorisee, 
  topic_info[, c("Topic", "label")], 
  by.x = "classe", 
  by.y = "Topic", 
  all.x = TRUE
)


base_categorisee <- subset(base_categorisee, select = -c(classe, proba))

```


```{r, warning=FALSE, message=FALSE}
doc_vides <- data.frame(
  id = Id_texte_NA
)


# 2. Identifier les noms des autres colonnes (sauf "document")
autres_colonnes <- setdiff(names(base_categorisee), "id")

# 3. Ajouter des NA pour les autres colonnes
doc_vides[autres_colonnes] <- NA

# 4. Fusionner avec la base existante
textes_by_topic_complet <- rbind(base_categorisee, doc_vides)

# 5. Optionnel: trier par document si n√©cessaire
textes_by_topic_complet <- textes_by_topic_complet[order(textes_by_topic_complet$id), ]

# Joindre les th√®mes dominants avec les tweets originaux

Texte_JI$Texte <- NULL

textes_classified <- Texte_JI %>%
  inner_join(textes_by_topic_complet, by = c("id" = "id"))

head(textes_classified)

```



```{r, warning=FALSE, message=FALSE, echo=FALSE}
# üìã Cr√©ation du tableau bas√© sur l'image
suggestions_table <- data.frame(
  id = 1:10,
  texte = c(
    NA,
    "Donner √† temps le micro aux personnes qui vont chanter.",
    NA,
    "Faire des sketchs courts et bien donner le micro aux acteurs.",
    "Int√©grer un court-m√©trage sur la diversit√© culturelle pour sensibiliser.",
    NA,
    "Faire des sketch courts et donner le micro aux acteurs sur la sc√®ne.",
    NA,
    "Am√©liorer le son pour que tout soit bien audible.",
    NA
  ),
  label = c(
    NA,
    "Gestion du timing lors des interventions",
    NA,
    "Sketchs et prestations",
    "Touche cr√©ative et courm√©trages",
    NA,
    "Sketchs et prestations",
    NA,
    "Aspect technique et organisation",
    NA
  )
)

# üñ®Ô∏è Affichage du tableau joliment format√©
knitr::kable(
  suggestions_table,
  caption = "üí° Suggestions pour am√©liorer l'organisation de la journ√©e d'int√©gration",
  align = "ccl"
)

```


\newpage
\section{CONCLUSION}\label{sec:importation}


L‚Äôanalyse des questions ouvertes √† l‚Äôaide des techniques de text mining met en lumi√®re l‚Äôimportance du **pr√©traitement** des donn√©es textuelles. Cette √©tape cruciale permet de nettoyer, normaliser et structurer le texte afin de le rendre exploitable pour les algorithmes d‚Äôanalyse. Cependant, il est important de noter que les outils de pr√©traitement sont plus adapt√©s et optimis√©s pour **l‚Äôanglais**, notamment en ce qui concerne les listes de **stop words**, les outils de **stemming** ou de **lemmatisation**. Cela constitue un frein lorsqu‚Äôon travaille sur des textes en fran√ßais ou dans d'autres langues moins repr√©sent√©es.

Parmi les m√©thodes explor√©es, **LDA (Latent Dirichlet Allocation)** permet d‚Äôidentifier des th√©matiques en se basant sur la fr√©quence des mots. Toutefois, cette approche pr√©sente des **limites importantes** : elle repose uniquement sur la **co-occurrence de mots**, sans prendre en compte leur sens r√©el ou leur contexte s√©mantique. Ainsi, des textes exprimant des id√©es similaires avec des mots diff√©rents peuvent ne pas √™tre associ√©s au m√™me th√®me, ce qui r√©duit la pertinence de l‚Äôanalyse dans certains cas.

C‚Äôest dans ce cadre que 
*BERTopic** se distingue. En s‚Äôappuyant sur des mod√®les d‚Äôembeddings comme BERT, il permet de capter la s√©mantique des phrases. Il devient alors possible de regrouper des textes similaires m√™me si les mots employ√©s sont diff√©rents. Cette approche offre une compr√©hension plus fine et plus pertinente des id√©es exprim√©es dans les donn√©es.

Cela dit, il est essentiel de garder √† l‚Äôesprit que le traitement des textes reste une t√¢che **complexe** et **imparfaite**. La diversit√© des styles, des formulations, des niveaux de langue ou encore des erreurs d‚Äô√©criture rend l‚Äôanalyse automatique difficile. Les r√©sultats doivent donc √™tre interpr√©t√©s avec pr√©caution, et id√©alement compl√©t√©s par une validation **humaine** pour garantir leur fiabilit√©.



# R√©f√©rences Bibliographiques

-   [Classification
    ](https://www.innovatiana.com/post/best-datasets-for-text-classification)
-   [Pr√©traitement
](https://guides.library.upenn.edu/penntdm/r)

-   [Pr√©traitement
](https://bookdown.org/tianyuan09/ai4ph2022/tutorial.html)
  
-   [Topic modeling
](https://ladal.edu.au/tutorials/topic/topic.html?)

