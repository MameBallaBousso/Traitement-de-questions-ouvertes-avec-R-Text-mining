text <- removeNumbers(text)     # Enlever les chiffres
text <- stripWhitespace(text)   # Supprimer les espaces en trop
return(text)
}
# Ajouter une nouvelle colonne "Texte_corrige"
data <- Texte_JI_filtered %>%
mutate(Texte_corrige = sapply(Texte, clean_text))
# Vérifier l'ajout de la colonne corrigée
glimpse(data)
tokenized_textes <- data %>%
select(id, Texte_corrige) %>%
unnest_tokens(input = 'Texte_corrige', output = 'word')
# Visualisation Tokenisation
tokenized_textes %>%
count(word, sort = TRUE) %>%
rename(count = n) %>%
filter(count > 5) %>%
mutate(word = reorder(word, count)) %>%
ggplot(aes(x = count, y = word)) +
geom_col()  +
labs(title = "Les mots apparaissant plus de 5 fois") +
scale_x_continuous(breaks = seq(0, 50, 5))
# Nombre total de lignes après tokenisation
nrow(tokenized_textes)
stop_words_fr <- tibble(word = stopwords("fr"))
Stemed_texte <- tokenized_textes %>%
anti_join(stop_words_fr, by = "word")
nrow(Stemed_texte)
View(Stemed_tweets)
View(tokenized_textes)
View(Stemed_texte)
Stemed_texte %>%
anti_join(stop_words_fr) %>% #finds where tweet words overlap with predefined stop words, and removes them
count(word, sort = TRUE) %>%
rename(count = n) %>%
filter(count > 5) %>%
mutate(word = reorder(word, count)) %>%
ggplot(aes(x = count, y = word)) +
geom_col() +
labs(title = "Count of Words in CCIDM Tweets") +
scale_x_continuous(breaks = seq(0, 50, 5))
library('ggwordcloud')
install.packages("ggwordcloud")
library(ggwordcloud)
Stemed_texte %>%
anti_join(stop_words_fr) %>%
count(word, sort = TRUE) %>%
filter(n > 4) %>%
ggplot(aes(label = word, size = n, color = n)) +
geom_text_wordcloud() +
scale_size_area(max_size = 15)
Stemed_texte %>%
count(word, sort = TRUE) %>%
rename(count = n) %>%
mutate(total=sum(count))%>%
mutate(tf=count/total) %>%
head()
texte_tf_idf <- Stemed_texte %>%
count(word, id, sort = TRUE) %>%
rename(count = n) %>%
bind_tf_idf(word, id, count)
head(texte_tf_idf)
texte_tf_idf %>%
select(word, id, tf_idf, count) %>%
group_by(id) %>%
slice_max(order_by = count, n = 6, with_ties=FALSE) %>% #takes top 5 words from each tweet
filter(tweet_id < 6) %>% #just look at 5 tweets
ggplot(aes(label = word)) +
geom_text_wordcloud() +
facet_grid(rows = vars(id))
texte_tf_idf %>%
select(word, id, tf_idf, count) %>%
group_by(id) %>%
slice_max(order_by = count, n = 6, with_ties=FALSE) %>% #takes top 5 words from each tweet
filter(id < 6) %>% #just look at 5 tweets
ggplot(aes(label = word)) +
geom_text_wordcloud() +
facet_grid(rows = vars(id))
texte_tf_idf %>%
select(word, id, tf_idf) %>%
group_by(id) %>%
slice_max(order_by = tf_idf,n = 6, with_ties=FALSE) %>% #takes top 5 words from each tweet
filter(tweet_id < 6) %>% #just look at 5 tweets
ggplot(aes(label = word)) +
geom_text_wordcloud() +
facet_grid(rows = vars(id))
texte_tf_idf %>%
select(word, id, tf_idf) %>%
group_by(id) %>%
slice_max(order_by = tf_idf,n = 6, with_ties=FALSE) %>% #takes top 5 words from each tweet
filter(id < 6) %>% #just look at 5 tweets
ggplot(aes(label = word)) +
geom_text_wordcloud() +
facet_grid(rows = vars(id))
glimpse(data)
textes_bigram <- data %>%
unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2)
head(tweets_bigram)
head(textes_bigram)
textes_bigram <- data %>%
select(id, Texte_corrige) %>%
unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2)
head(textes_bigram)
stop_words_fr <- tibble(word = stopwords)
View(stop_words_fr)
textes_bigram <- textes_bigram %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%#separates on whitespace
filter(!word1 %in% stop_words_fr$word) %>%
filter(!word2 %in% stop_words_fr$word)
head(tweets_bigram)
head(textes_bigram)
bigram_counts <- textes_bigram %>%
count(word1, word2, sort = TRUE)
head(bigram_counts)
View(bigram_counts)
View(textes_bigram)
data %>%
select(id, Texte_corrige) %>%
unnest_tokens(bigram, Texte_corrige, token = 'ngrams', n = 2) %>%
count(id, bigram) %>%
bind_tf_idf(bigram, id, n) %>%
group_by(id) %>%
arrange(id, desc(tf_idf)) %>%
head()
install.packages("igraph")
install.packages("ggraph")
library('igraph')
library('ggraph')
bi_graph <- bigram_counts %>%
filter(n > 2) %>%
graph_from_data_frame()
ggraph(bi_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
bi_graph <- bigram_counts %>%
filter(n > 1) %>%
graph_from_data_frame()
ggraph(bi_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
tweets_trigram <- data %>%
unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% #separates on whitespace
filter(!word1 %in% stop_words_fr$word) %>%
filter(!word2 %in% stop_words_fr$word) %>%
filter(!word3 %in% stop_words_fr$word)
head(tweets_trigram)
texte_trigram <- data %>%
select(id, Texte_corrige) %>%
unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% #separates on whitespace
filter(!word1 %in% stop_words_fr$word) %>%
filter(!word2 %in% stop_words_fr$word) %>%
filter(!word3 %in% stop_words_fr$word)
head(texte_trigram)
trigram_counts <- tweets_trigram %>%
count(word1, word2, word3, sort = TRUE)
head(trigram_counts)
data %>%
select(id, Texte_corrige) %>%
unnest_tokens(trigram, Texte_corrige, token = 'ngrams', n = 3) %>%
count(id, trigram) %>%
bind_tf_idf(trigram, id, n) %>%
group_by(id) %>%
arrange(id, desc(tf_idf)) %>%
head()
tri_graph <- trigram_counts %>%
filter(n > 1) %>%
graph_from_data_frame()
ggraph(tri_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
tri_graph <- trigram_counts %>%
graph_from_data_frame()
ggraph(tri_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
tri_graph <- trigram_counts %>%
filter(n > 0) %>%
graph_from_data_frame()
ggraph(tri_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
View(Texte_JI_filtered)
View(tokenized_textes)
View(bigram_counts)
Stop_texte <- tokenized_textes %>%
anti_join(stop_words_fr, by = "word")
nrow(Stop_texte)
Stop_texte %>%
anti_join(stop_words_fr) %>% #finds where tweet words overlap with predefined stop words, and removes them
count(word, sort = TRUE) %>%
rename(count = n) %>%
filter(count > 5) %>%
mutate(word = reorder(word, count)) %>%
ggplot(aes(x = count, y = word)) +
geom_col() +
labs(title = "Count of Words in CCIDM Tweets") +
scale_x_continuous(breaks = seq(0, 50, 5))
Stop_texte %>%
anti_join(stop_words_fr) %>% #finds where tweet words overlap with predefined stop words, and removes them
count(word, sort = TRUE) %>%
rename(count = n) %>%
filter(count > 5) %>%
mutate(word = reorder(word, count)) %>%
ggplot(aes(x = count, y = word)) +
geom_col() +
labs(title = "Les mots apparaissant plus de 5 fois") +
scale_x_continuous(breaks = seq(0, 50, 5))
Stop_texte %>%
anti_join(stop_words_fr) %>%
count(word, sort = TRUE) %>%
filter(n > 4) %>%
ggplot(aes(label = word, size = n, color = n)) +
geom_text_wordcloud() +
scale_size_area(max_size = 15)
Stop_texte %>%
count(word, sort = TRUE) %>%
rename(count = n) %>%
mutate(total=sum(count))%>%
mutate(tf=count/total) %>%
head()
texte_tf_idf <- Stop_texte %>%
count(word, id, sort = TRUE) %>%
rename(count = n) %>%
bind_tf_idf(word, id, count)
head(texte_tf_idf)
Stop_texte %>%
count(word, sort = TRUE)%>%
nrow()
Stop_texte = Stop_texte %>%
mutate(stem = wordStem(word))
Stop_texte
Stop_texte %>%
count(stem, sort = TRUE) %>%
nrow()
Stop_texte %>%
count(word) %>%
arrange(desc(n)) %>%
head(10)
Stop_texte %>%
count(stem) %>%
arrange(desc(n)) %>%
head(10)
View(Stop_texte)
ud_model <- udpipe_load_model("french-gsd-ud-2.5-191206.udpipe")
ud_model <- udpipe_load_model("french-gsd-ud-2.5-191206.udpipe")
library(udpipe) # Pour la lemmatisation
ud_model <- udpipe_load_model("french-gsd-ud-2.5-191206.udpipe")
ud_model <- udpipe_load_model("french-gsd-ud-2.5-191206.udpipe")
text_grouped <- Stop_texte %>%
group_by(id) %>%
summarise(text = paste(word, collapse = " "), .groups = "drop")
ud_model <- udpipe_load_model("french-gsd-ud-2.5-191206.udpipe")
# Charger les bibliothèques nécessaires
library(readxl)
library(topicmodels)
library(ggplot2)
library(dplyr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(tidyverse)
library(tm)
library(SnowballC)
library(stringr)
library(udpipe) # Pour la lemmatisation
Enquête_dopinion_relative_à_la_journée_dintégration_ <- read_excel("Data/Enquête_dopinion_relative_à_la_journée_dintégration).xlsx")
View(Enquête_dopinion_relative_à_la_journée_dintégration_)
tweetsDF <- Enquête_dopinion_relative_à_la_journée_dintégration_
colnames(tweetsDF)
glimpse(tweetsDF)
nrow(tweetsDF) # check the # of rows
table(tweetsDF$id) # build a contingency table of counts
# Fonction de nettoyage du texte
clean_text <- function(text) {
text <- tolower(text)           # Minuscule
text <- removePunctuation(text) # Enlever la ponctuation
text <- removeNumbers(text)     # Enlever les chiffres
text <- stripWhitespace(text)   # Supprimer les espaces en trop
return(text)
}
# Ajouter une nouvelle colonne "Texte_corrige"
data <- tweetsDF %>%
mutate(Texte_corrige = sapply(Texte, clean_text))
# Vérifier l'ajout de la colonne corrigée
# Tokeniser la colonne Texte_corrige
text_df <- data %>%
select(id, Texte_corrige) %>%
unnest_tokens(word, Texte_corrige)
# Identifier les documents vides ou avec seulement NA
text_empty <- text_df %>%
group_by(id) %>%
summarise(nb_mots = sum(!is.na(word))) %>%
filter(nb_mots == 0) %>%
pull(id)
# Garde uniquement les documents non vides pour le LDA
text_filtered <- text_df %>%
filter(!(id %in% text_empty))
# Afficher les premières lignes
head(text_filtered)
# Nombre total de lignes après tokenisation
nrow(text_filtered)
# Charger les stop words en français
stop_words_fr <- tibble(word = stopwords("fr"))
# Nombre total de lignes avant suppression
nrow(text_filtered)
# Supprimer les mots vides en français
text_filtered <- text_filtered %>%
anti_join(stop_words_fr, by = "word")
# Nombre total de lignes après suppression
nrow(text_filtered)
# unique count of words before stemming
text_filtered %>%
count(word, sort = TRUE)%>%
nrow()
# Le nombre de mots unique par tweet
# stemming
text_filtered = text_filtered %>%
mutate(stem = wordStem(word))
# unique count of words after stemming
text_filtered %>%
count(stem, sort = TRUE) %>%
nrow()
# Fréquence des mots avant le stemming
text_filtered %>%
count(word) %>%
arrange(desc(n)) %>%
head(10)
# Fréquence des mots après le stemming
text_filtered %>%
count(stem) %>%
arrange(desc(n)) %>%
head(10)
ud_model <- udpipe_load_model("french-gsd-ud-2.5-191206.udpipe")
texte_tf_idf <- Stop_texte %>%
count(word, id, sort = TRUE) %>%
rename(count = n) %>%
bind_tf_idf(word, id, count)
head(texte_tf_idf)
Id_initial_texte <- unique(Texte_JI$id)
Id_initial_texte <- unique(Texte_JI$id)
length(Id_initial_texte)
Id_final_texte <- unique(Stop_texte$id)
length(Id_final_texte)
Id_texte_NA <- setdiff(Id_initial_texte, Id_final_texte)
Id_texte_NA
df_dtm <- Stop_texte %>%
count(id, word) %>%
cast_dtm(id, word, n)
k_values <- 2:15
perplexities <- sapply(k_values, function(k) {
lda_model <- LDA(df_dtm, k = k, control = list(seed = 1234))
perplexity(lda_model)
})
ggplot(data.frame(K = k_values, Perplexity = perplexities), aes(x = K, y = Perplexity)) +
geom_point() + geom_line() +
labs(title = "Choix du K optimal avec la perplexité",
x = "Nombre de thèmes (K)", y = "Perplexité")
lda_model <- LDA(df_dtm1, k = 8, control = list(seed = 1234))
lda_model <- LDA(df_dtm, k = 8, control = list(seed = 1234))
terms_by_topic <- tidy(lda_model, matrix = "beta")
terms_by_topic
top_terms <- terms_by_topic %>%
group_by(topic) %>%
slice_max(beta, n = 10, with_ties = FALSE) %>%  # Prend exactement 5 termes par thème
ungroup() %>%
arrange(topic, -beta)  # Trie par thème et par probabilité décroissante
top_terms %>%
count(topic)
head(terms_by_topic)
terms_by_topic
top_terms <- terms_by_topic %>%
group_by(topic) %>%
slice_max(beta, n = 10, with_ties = FALSE) %>%  # Prend exactement 10 termes par thème
ungroup() %>%
arrange(topic, -beta)  # Trie par thème et par probabilité décroissante
# Vérification du nombre de termes sélectionnés par thème
top_terms %>%
count(topic)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(title = "Termes les plus probables par thème",
x = "Probabilité (beta)", y = "Terme")
textes_gamma <- tidy(lda_model, matrix = "gamma")
textes_gamma
textes_classified <- textes_gamma %>%
group_by(document) %>%
slice_max(gamma) %>%
ungroup()
textes_classified <- textes_gamma %>%
group_by(document) %>%
slice_max(gamma) %>%
ungroup()
textes_classified %>%
count(topic)
textes_classified %>%
ggplot(aes(factor(topic), fill = factor(topic))) +
geom_bar() +
labs(title = "Distribution des thèmes dominants",
x = "Thème", y = "Nombre de textes")
top_terms <- terms_by_topic %>%
group_by(topic) %>%
slice_max(beta, n = 10, with_ties = FALSE) %>%  # Prend exactement 10 termes par thème
ungroup() %>%
arrange(topic, -beta)  # Trie par thème et par probabilité décroissante
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(title = "Termes les plus probables par thème",
x = "Probabilité (beta)", y = "Terme")
textes_gamma <- textes_gamma %>%
mutate(topic_label = case_when(
topic == 1 ~ "Diversité culturelle et activités communes",
topic == 2 ~ "Performances artistiques et intervention scénique",
topic == 3 ~ "Présentation des cultures par les communautés",
topic == 4 ~ "Aménagement de l'espace et la gestion du temps",
topic == 5 ~ "Organisation",
topic == 6 ~ "Suggestions d'amélioration",
topic == 7 ~ "Organisation générale et impression globale",
topic == 8 ~ "animation",
TRUE ~ "Autre"
))
tweets_by_topic <- textes_gamma %>%
mutate(document = as.integer(document)) %>%
group_by(document) %>%
slice_max(gamma, n = 1)
glimpse(tweets_by_topic)
doc_vides <- data.frame(
document = Id_texte_NA
)
autres_colonnes <- setdiff(names(tweets_by_topic), "document")
doc_vides[autres_colonnes] <- NA
tweets_by_topic_complet <- rbind(tweets_by_topic, doc_vides)
tweets_by_topic_complet <- tweets_by_topic_complet[order(tweets_by_topic_complet$document), ]
textes_classified <- Texte_JI %>%
inner_join(tweets_by_topic_complet, by = c("id" = "document"))
head(textes_classified, 10)
View(textes_classified)
library(reticulate)
py_config()
py_config()
sentence_transformers <- import("sentence_transformers")
reticulate::py_install(
packages = c("sentence-transformers", "hdbscan", "umap-learn", "bertopic"),
pip = TRUE
)
library(reticulate)
reticulate::py_install(
packages = c("sentence-transformers", "hdbscan", "umap-learn", "bertopic"),
pip = TRUE
)
py_config()
# Installation des packages nécessaires dans l’environnement actif de reticulate
reticulate::py_install(
packages = c("sentence-transformers", "hdbscan", "umap-learn", "bertopic"),
pip = TRUE
)
library(reticulate)
install_miniconda()
use_condaenv()
library(reticulate)
use_condaenv()
use_condaenv(r-reticulate)
use_condaenv("r-reticulate")
reticulate::py_install(
packages = c("sentence-transformers", "hdbscan", "umap-learn", "bertopic"),
pip = TRUE
)
py_config()
# 📦 Imports des modules Python
sentence_transformers <- import("sentence_transformers")
hdbscan <- import("hdbscan")
bertopic <- import("bertopic")
umap <- import("umap")  # important pour fixer le random_state
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-MiniLM-L6-v2")
hdbscan_model <- hdbscan$HDBSCAN(
min_cluster_size = reticulate::r_to_py(3L),
min_samples = reticulate::r_to_py(1L)
)
umap_model <- umap$UMAP(
n_neighbors = 15L,
n_components = 5L,
min_dist = 0.0,
metric = "cosine",
random_state = 42L  # ✅ Seed fixée ici
)
umap_model <- umap$UMAP(
n_neighbors = 15L,
n_components = 5L,
min_dist = 0.0,
metric = "cosine",
random_state = 42L  # ✅ Seed fixée ici
)
topic_model <- bertopic$BERTopic(
language = "french",
embedding_model = embedding_model,
hdbscan_model = hdbscan_model,
umap_model = umap_model
)
