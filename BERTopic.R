library(reticulate)

# ğŸ“¦ Imports des modules Python
sentence_transformers <- import("sentence_transformers")
hdbscan <- import("hdbscan")
bertopic <- import("bertopic")
umap <- import("umap")  # ğŸ‘ˆ important pour fixer le random_state



# ğŸ”¤ ModÃ¨le d'embedding (changeable par d'autres plus bas)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-MiniLM-L6-v2")


# Les variÃ©tÃ©s existents du modÃ¨le (plus puissants) :

# "paraphrase-distilbert-base-nli-stsb" : (taille : 768 | Bonne prÃ©cision sÃ©mantique
# "bert-base-nli-mean-tokens" : (taille : 768 | TrÃ¨s bonne prÃ©cision sÃ©mantique)
# "all-mpnet-base-v2" : (taille: 768 | Excellente prÃ©cision sÃ©mantique)

# ğŸ”§ Clustering HDBSCAN
hdbscan_model <- hdbscan$HDBSCAN(
  min_cluster_size = reticulate::r_to_py(3L),
  min_samples = reticulate::r_to_py(1L)
)

# ğŸ¯ RÃ©duction de dimension via UMAP avec seed fixÃ©e pour reproductibilitÃ©
umap_model <- umap$UMAP(
  n_neighbors = 15L,
  n_components = 5L,
  min_dist = 0.0,
  metric = "cosine",
  random_state = 42L  # âœ… Seed fixÃ©e ici
)

# ğŸ“š CrÃ©ation du modÃ¨le BERTopic
topic_model <- bertopic$BERTopic(
  language = "french",
  embedding_model = embedding_model,
  hdbscan_model = hdbscan_model,
  umap_model = umap_model
  )



text_empty <- tweetsDF %>%
  group_by(id) %>%
  summarise(nb_mots = sum(!is.na(Texte))) %>%
  filter(nb_mots == 0) %>%
  pull(id)

# Garde uniquement les documents non vides pour le LDA
text_filtered <- tweetsDF %>% 
  filter(!(id %in% text_empty))
View(text_filtered)


# ğŸ“¦ PrÃ©paration des donnÃ©es
docs <- text_filtered$Texte
ids <- text_filtered$id  # on garde l'id associÃ© Ã  chaque texte


# ğŸ§  Application du modÃ¨le
result <- topic_model$fit_transform(docs)


# ğŸ¯ Extraction des rÃ©sultats
topics <- result[[1]]
probs <- result[[2]]


# â• Ajout des rÃ©sultats au data.frame dâ€™origine
text_filtered$topic <- topics
text_filtered$topic_proba <- probs


# ğŸ” Reconstruction du data.frame avec id + texte + classe
base_categorisee <- data.frame(
  id = ids,
  texte = docs,
  classe = topics,
  proba = probs
)

View(base_categorisee)


# ğŸ” Affichage des infos sur les thÃ¨mes trouvÃ©s
topic_info <- topic_model$get_topic_info()
View(topic_info)

topic_info$label <- c(
  "Suggestions diverses",                                    # Topic -1
  "Sketchs et scÃ¨nes courtes",                                                  # Topic 0
  "Elargir le restaurant Salle",                                  # Topic 1
  "Culture et traditions des pays",                           # Topic 2
  "Organisation et co-animation",                             # Topic 3
  "AmÃ©nagement de lâ€™espace et durÃ©e",                         # Topic 4
  "QualitÃ© du son"                                         # Topic 5
                                                   # Topic 8
)


base_categorisee <- merge(
  base_categorisee, 
  topic_info[, c("Topic", "label")], 
  by.x = "classe", 
  by.y = "Topic", 
  all.x = TRUE
)


base_categorisee <- subset(base_categorisee, select = -c(classe, proba))


doc_vides <- data.frame(
  id = text_empty
)
# 2. Identifier les noms des autres colonnes (sauf "document")
autres_colonnes <- setdiff(names(base_categorisee), "id")

# 3. Ajouter des NA pour les autres colonnes
doc_vides[autres_colonnes] <- NA

# 4. Fusionner avec la base existante
tweets_by_topic_complet <- rbind(base_categorisee, doc_vides)

# 5. Optionnel: trier par document si nÃ©cessaire
tweets_by_topic_complet <- tweets_by_topic_complet[order(tweets_by_topic_complet$id), ]

# Joindre les thÃ¨mes dominants avec les tweets originaux
tweets_classified <- tweetsDF %>%
  inner_join(tweets_by_topic_complet, by = c("id" = "id"))

# Afficher les 10 premiers tweets avec leur thÃ¨me dominant et leur libellÃ©
head(tweets_classified, 10)

View(tweets_classified)

